#+TITLE: Last Chapter
#+AUTHOR: Rick Neff
#+EMAIL: rick.neff@gmail.com
#+LANGUAGE: en
#+OPTIONS: H:4 num:nil toc:t \n:nil @:t ::t |:t ^:t *:t TeX:t LaTeX:t ':nil d:t
#+STARTUP: showeverything entitiespretty
#+SETUPFILE: theme-readtheorg.setup

* TODO Add this exercise somewhere                                :noexport:
:EXERCISE:
   The operation called *dehydration* takes a word and deletes all letters in it
   from H to O. What is the original (rehydrated) text of this dehydrated sequence
   of words?

   TE TE AS CE TE WARUS SAID T TA OF AY TGS F SES AD SPS AD SEAG WAX F CABBAGES
   AD KGS AD WY TE SEA S BG T AD WETER PGS AVE WGS
  
   :ANSWER:
   THE TIME HAS COME THE WALRUS SAID TO TALK OF MANY THINGS OF SHOES AND SHIPS
   AND SEALING WAX OF CABBAGES AND KINGS AND WHY THE SEA IS BOILING HOT AND
   WHETHER PIGS HAVE WINGS
   :END:
:END:

* TODO Also this one                                              :noexport:
:EXERCISE:
   What English word has the smallest vowel-to-consonant ratio? For example, the
   v2c ratios of the words in the previous sentence are 1/3, 2/5, 1/3, 1/2, 1/2,
   1/3, 2/3, 1/1, 1/2, and 3/2. No clear winner. Count =y= as a vowel, even when
   it is the first letter of a word.

   :ANSWER:
   The word strengths has a 1/8 v2c ratio.
   :END:
:END:

* TODO Check out the following files/items                         :noexport:
  [[http://julienchastang.com/dotemacs.html]]
  [[http://orgmode.org/worg/org-contrib/org-drill.html]]
  Follow this link to execute =(org-agenda)=: [[elisp:org-agenda][Org Agenda]]
  Learned from org-mode talk by Harry Schwartz:   
#+BEGIN_SRC emacs-lisp
  (org-babel-load-file "configuration.org")
#+END_SRC
** TODO Look at Getting Things Done book (GTD)
*** Look at Jeff Atwood (Trash Your TODO List)
*** ownCloud (like a DropBox/Google Drive replacemet)
*** Org Capture Templates
*** ABook integrates with Mutt (Org Contacts -- or bbdb)
*** Package org2blog (org document to Wordpress)
*** Package org-publish 
*** ox-twbs (Org export Twitter Bootstrap)
*** ox-gfm (Github Flavored Markdown)
*** org-drill (flash cards)
*** Orgzly (on Android)
*** Drafts (on iOS)
*** entr (generalized iNotify tool)
*** Recognize the arguments in [[https://www.youtube.com/watch?v=QM1iUe6IofM][OOP is Bad]] by Brian Will
* THR

** YZ@

  \ldquo{}Isn\rsquo{}t it interesting how languages either facilitate or impede
  communication?\rdquo asked Til, just as Ila and Abu entered his den. Ila replied,
  without missing a beat, \ldquo{}I know programming languages facilitate communication
  with computers, but I don\rsquo{}t see the impediment\rdquo{}. \ldquo{}I do,\rdquo said Abu. \ldquo{}Not
  knowing any programming languages, except a little lisp, I suppose, impedes my
  communication with you when you\rsquo{}re talking programming.\rdquo \ldquo{}Yes\rdquo, said Ila, \ldquo{}but
  ignorance on your part is not the fault of the languages. So, Til, did you
  mean ignorance of languages is the impediment?\rdquo

  \ldquo{}No, not necessarily. Sometimes a perfectly natural and known language like
  English is used, or I should say /misused/, to obfuscate rather than
  communicate --- to hide rather than reveal meaning. I\rsquo{}ll come back to this
  point later.\rdquo

  \ldquo{}So the question, what is a language, with its obvious answer, a language is
  something used to communicate, is too general in one sense, and too specific
  in another.\rdquo

  \ldquo{}As you\rsquo{}ll see, languages can be generalized to a set of strings, but made more
  specific by /how/ that set is defined. What are called Phrase-Structure
  Grammars --- PSGs --- are the tools (rules) used to define which strings are
  valid members of the language associated with that PSG. Now let me be clear
  what that association is. It is that PSGs /generate/ languages, and so of
  course, languages are /generated/ (built) by PSGs.\rdquo

  \ldquo{}In English (or any natural, human language) you follow the rules when you
  adhere to the grammar. A main component of a formally defined grammar is the
  set of productions --- the rules that /produce/ valid strings in the language.
  In essence, applying these rules is how different strings of words (phrases,
  sentences) are generated. Deliberately, or even ignorantly violating these
  rules leads to confusion and failed communication. Because the grammatical
  rules of English are so complex, ambiguity and misunderstanding frequently
  lurk beneath the surface.\rdquo

  \ldquo{}Take what you said earlier, Ila. \lsquo{}I know programming languages facilitate X.\rsquo
  (I\rsquo{}m abbreviating.) One sentence, or two?\rdquo

: I know programming. Languages facilitate X.

  Abu said, \ldquo{}It seems pretty clear from the context that it\rsquo{}s one sentence.\rdquo Ila
  said, \ldquo{}I agree. I don\rsquo{}t see the problem.\rdquo

  \ldquo{}It\rsquo{}s *not* a problem --- for you. Missing the context, and minus the fluidity
  and rapidity with which you uttered that *one* sentence, however, it could be
  misconstrued --- legally but erroneously parsed --- into two.\rdquo

  Til cleared his throat and went on. \ldquo{}Take this imperative statement: /Love One
  Another./ A Christian maxim, if I\rsquo{}m not mistaken.\rdquo

  Ila and Abu glanced at each other in surprise. Til, \lsquo{}til now, had never
  brought up religion in any of their discussions, whereas *they* had certainly
  broached the subject, on more than one occasion, when it was just the two of
  them, meeting to work on homework assignments, or just waiting for Til. Ila
  thought Abu, a Mormon, hopelessly deluded. Abu thought Ila, an evangelical
  Christian, a prospective Mormon, if she could only get past her myopia! Now to
  hear Til bring this up was a tantalizing turn, and mildly amusing to all three
  of them when Ila and Abu said in unison, \ldquo{}John 13:34!\rdquo

  \ldquo{}Yes, well, very good,\rdquo said Til, with raised eyebrows. \ldquo{}I see you both know
  your Bible.\rdquo If they think, thought Til, that this is leading into a religious
  discussion, they will be disappointed. Not that he was opposed to hearing
  their religious opinions, just not right now. (When, then? asked a nagging
  voice in his head. He ignored it. But God was still to be the subject of a
  sentence.)

  \ldquo{}This is not the simplest English grammatical construction. Simpler would be
  \lsquo{}God Loves You,\rsquo which is in Subject-Verb-Object order, very common in English.
  You could make sense of each of the other five permutations, but only by
  giving a lot of context:\rdquo

  1. Love Another One.
  2. One Love Another.
  3. One Another Love.
  4. Another Love One.
  5. Another One Love.

  \ldquo{}So,\rdquo said Ila, \ldquo{}are you saying word order causes communication problems?\rdquo

  \ldquo{}Here\rsquo{}s a thought,\rdquo said Abu. \ldquo{}Have you ever tried to make sense of some kinds
  of poetry? Poets are always scrambling word order to fit some rhyme or meter
  scheme. Like in [[a poem by Wordsworth][a poem by Wordsworth]] that I just read:\rdquo

#+BEGIN_VERSE
  Bliss was it in that dawn to be alive.
  But to be young was very Heaven!
#+END_VERSE

  Said Til, \ldquo{}Or from a more familiar poem: \lsquo{}Quoth the raven\rsquo instead of \lsquo{}The raven
  said.\rsquo Still perfectly correct. In fact, saying \lsquo{}The raven quoth\rsquo is *not*
  grammatical, by the very special rule attached to that very archaic word.\rdquo

  \ldquo{}So, not so much word order as word choice. Remember I started talking about
  the misuse of English to obfuscate? *Jargon* is the apposite word here. This
  misuse of language erects barriers in the guise of facilitators. We justify
  ourselves in this activity because of the benefits --- and there *are*
  benefits --- but do we ever weigh them against the costs?\rdquo

  \ldquo{}Benefit-wise, jargon saves time and space by compressing information that
  needs sharing, but somewhat-intangible-cost-wise that very efficiency impedes
  understanding by outsiders of what insiders are saying. Hiding meaning from
  the uninitiated is such a powerful urge, and human vanity is an imp that eggs
  us on.\rdquo

  \ldquo{}For example, what does partial template specialization mean? And what is the
  cardinality of the power set?\rdquo

  Said Ila, \ldquo{}I can tell you the answer to your second question, but I have no
  clue about the first.\rdquo

  Said Abu, \ldquo{}I agree, but regarding your second question, do you think it\rsquo{}s
  better to put it more simply, like, how many subsets does a set of size *n*
  have?\rdquo

  \ldquo{}I do,\rdquo said Til. \ldquo{}Math is a language that desperately needs less jargon, more
  clarity. And not to keep you in suspense, well, not to keep you from the
  adventure of discovery, either, so for a little hint, partial template
  specialization is a very obscure quote-unquote feature of the C++ language.\rdquo

  Ila said, \ldquo{}I'll check it out, but just from the sound of it I'm glad C++ is
  not the language my company uses!\rdquo Abu added, \ldquo{}Me too!\rdquo Ila said, \ldquo{}You run a
  nursery, what are you talking about?\rdquo \ldquo{}Well,\rdquo said Abu, \ldquo{}we just hired a
  programmer to help us manage our inventory and production.\rdquo Til interrupted,
  \ldquo{}Let's talk about the programming language choice issue later.\rdquo

  \ldquo{}But speaking of business, you\rsquo{}ve no doubt heard the stories about when a
  business consultant, tongue firmly in cheek --- or not --- randomly chooses
  three words from three different lists to create for client consideration
  impressive-sounding, meaningless phrases, like

  customer value trajectory, or stratified business intelligence, or hypercubic
  mission criticality.\rdquo

  \ldquo{}Wow, did you just make those up?\rdquo wondered Abu, silently. Ila said, \ldquo{}I hear
  that stuff all the time from the consultants my company hires. It\rsquo{}s worse than
  nonsense, if you ask me.\rdquo

  \ldquo{}But not all of it is so obviously bad,\rdquo said Til. \ldquo{}Let me put it this way.
  Proclivities --- what a nice word! Many people have proclivities,
  inclinations, predispositions to use more words, or bigger words, or *shinier*
  words than necessary to get what they want, or what they think they want.
  Flattery is replete with this abuse of language.\rdquo

  Abu rose to the challenge: \ldquo{}Your mellifluous speech shows a penchant for
  pulchritudinous word marshalling.\rdquo

  Ila snorted. \ldquo{}You mean /marshmallowing/ --- sicky sweet, with no nutritional
  value!\rdquo

  \ldquo{}So you agree it\rsquo{}s a problem!\rdquo winked Til. Both Abu and Ila nodded and
  chuckled.

  \ldquo{}Well, it\rsquo{}s not one we\rsquo{}re going to solve today,\rdquo he said. \ldquo{}So let\rsquo{}s go back to
  talking about problems in mathematics. Mathematical language, unlike natural
  language, is precise and unambiguous. Equations --- tautologies --- always
  true. Never a doubt. Pure syntax without the clouding confusion of semantics.\rdquo

  \ldquo{}That\rsquo{}s the official story. Now let me qualify that a bit. Kurt G\ouml{}del, one of
  the, if not *the* most brilliant mathematical logicians of all time, once
  said:

#+BEGIN_QUOTE
  The more I [[think about language][think about language]], the more it amazes me that people ever
  understand each other.
#+END_QUOTE

  \ldquo{}What amazes me about mathematicians, who are people too, is that they are
  such poor writers --- when writing mathematics, at least. I alluded to this a
  little bit ago. [[Math writing is notorious][Math writing is notorious]] for its lack of clarity, despite its
  claim of delivering unadulterated truth.\rdquo

  His excitement bubbling, Abu said, \ldquo{}I like this quote from one of the books you
  recommended that I just started reading. The authors say \dots\rdquo

#+BEGIN_QUOTE
  What we present may not resemble math, because we avoid the cryptic equations,
  formulas, and graphs that many people have come to know and fear as
  mathematics. Indeed, those symbols are the memorable icons of an
  often-forbidding [[foreign language of mathematical jargon][foreign language of mathematical jargon]], but it\rsquo{}s not the
  only language of mathematics and it does not reside at the center of the
  subject. The deepest and richest realms of mathematics are often devoid of the
  cryptic symbols that have baffled students through the generations. Ideas ---
  intriguing, surprising, fascinating, and beautiful --- are truly at the heart
  of mathematics.
#+END_QUOTE

  Ila said, \ldquo{}I have a quote, too, along these lines. May I share it?\rdquo Til nodded,
  and Abu winced --- was Ila jabbing him for plunging ahead without asking?

  \ldquo{}It\rsquo{}s also from one of your recommended books. The author is a Nobel laureate
  physicist:\rdquo

#+BEGIN_QUOTE
  To many people who are not physicists, modern physics seems to have left the
  solid world of understandable here-and-now to enter a weird realm of
  uncertainties and strange, ephemeral particles that have whimsical names and
  dubious existence. What has actually happened is that physics has gone far
  beyond the point where ordinary, everyday experiences can provide a kind of
  human analogy to the things that the physicists are studying. It is a problem
  of language. The [[vocabulary and syntax of human language][vocabulary and syntax of human language]] evolved to describe
  the workings of the everyday world, the world we can see, hear, touch, taste
  and smell. Words were simply not intended to describe things unimaginably
  great or incredibly small, far beyond the range of our unaided senses, where
  the rules of the game are changed. The true language of physics is
  mathematics.
#+END_QUOTE

  Said Til: \ldquo{}Excellent! But more on these ideas later. Think about what you know
  about the language of logic. It had something of a learning curve when you
  first encountered it, right? Formal logic is a formidable but foundational
  system of thought, a way to give /precision/ to thought and reasoning, that
  can nonetheless trip up the uninitiated. Since I just mentioned Kurt G\ouml{}del,
  let me give you [[a description of formal systems][a description of formal systems]], or at least, the /rules/ of
  formal systems, from a book written about him.

  \ldquo{}This passage pauses while expressing the point of view that \lsquo{}mathematics is
  merely syntactic;\rsquo\rdquo

#+BEGIN_QUOTE
  its truth derives from the rules of formal systems, which are of three basic
  sorts: the rules that specify what the symbols of the system are (its
  \ldquo{}alphabet\rdquo); the rules that specify how the symbols can be put together into
  what are called well-formed formulas, standardly abbreviated \ldquo{}wff,\rdquo and
  pronounced \ldquo{}woof\rdquo; and the rules of inference that specify which wffs can be
  derived from which.
#+END_QUOTE

  Said Abu, \ldquo{}I recall some of the rules we learned for the wffs of propositional
  logic, like \lsquo{}p \rarr (q \vee r)\rsquo is one but \lsquo\rarr p (\vee q r)\rsquo is not.\rdquo

  Said Ila, \ldquo{}Unless you put parentheses around the whole thing and call it
  lisp!\rdquo

  Said Til, \ldquo{}Right! You would have to scramble the symbols even more, like \lsquo{}p q ) \vee (r
  \rarr\rsquo to really unwffify it!\rdquo

  Said Abu, \ldquo{}So like the shuffling you did with the Christian maxim, as you called
  it, would you say you unwffified that phrase five different ways?\rdquo

  Said Til, \ldquo{}The rules of English grammar are many and varied, and sometimes
  downright mysterious, so I will leave that question for you to answer!
  However, a wff like \lsquo{}he went to the store\rsquo is quite obviously *not* well
  formed as \lsquo{}to he store the went\rsquo.\rdquo

  Said Ila, \ldquo{}So the rules we learned for composing propositions --- there were only
  four of them, right?\rdquo

  Said Til, \ldquo{}Yes. But that simple grammar did not take into account parentheses
  for grouping, so it was incomplete. But we will fix that later. There\rsquo{}s one
  more thing I want to mention about human communication before shifting our
  focus a bit. We don\rsquo{}t stop to consider this very often, but another problem
  (bug? feature?) of language is the problem of linearity --- or sequentiality
  --- the sentences as \lsquo{}[[beads on a string][beads on a string]]\rsquo analogy used by Steven Pinker
  and others --- words must be written out or spoken and then read or heard in
  sequence, over time, instead of just apprehended \lsquo{}all at once\rsquo --- /in toto/.
  What would our communication be like if we could do that?\rdquo

  Abu and Ila looked deep in thought, grappling with that idea, which pleased
  Til, but he also knew that they didn\rsquo{}t have time to discuss it --- not today.
  Rather reluctantly, he interrupted their reverie and said, \ldquo{}Hold those
  thoughts, and we\rsquo{}ll dissect them later. For now, other problems await our
  attention. While obviously mathematical in nature, indeed, *discrete*
  mathematical, problems in computer science are the ones we will focus on.\rdquo

  \ldquo{}Computer scientists, especially those into theoretical computer science, like
  to cast problems in the common mold of languages. They do this for technical
  reasons, more thoroughly delved into in a course on computational theory. But
  here is a simple, favorite example: Is 23 prime? This is a decision problem
  whose answer is yes, easily verified by [[simply trying to divide 23 by 2 and 3][simply trying to divide 23 by 2 and 3]],
  and failing on both counts, of course. This decision could *also* be made by
  sequentially searching for and finding the string 

: "23"

  in the set

: ["2" "3" "5" "7" "11" "13" "17" "19 "23" ...]

  of strings.\rdquo{}

#+BEGIN_SRC emacs-lisp
  (format "%S" (number-to-string 23))
#+END_SRC

: "23"

#+BEGIN_SRC emacs-lisp :results raw
  (format "%S" (member (number-to-string 23)
                       (map 'list 'number-to-string [2 3 5 7 11 13 17 19 23])))
#+END_SRC

: ("23")

  Til went on. \ldquo{}This set of strings is a language, and if you allow that the
  \lsquo{}...\rsquo stands for an infinity of bigger and bigger strings of this rather
  well-known kind, it is the language of PRIMES. It is given the name PRIMES, at
  any rate. So, does PRIMES contain the string "23232323232323232323"? is
  another way to ask, is 23232323232323232323 prime? The answer is no --- it\rsquo{}s a
  composite number with seven prime factors --- including 23 --- but the
  computational solution to that set membership determination problem is
  significantly harder than the one for 23. It\rsquo{}s not done by simply searching in
  a static list. While many lists of primes exist, no one creates lists with
  every prime in it up to some huge limit. True, programs exist that can do
  that, using some variation of the classic [[Sieve of Eratosthenes][Sieve of Eratosthenes]], which goes
  *way* back, showing how old this problem is. But the point is, to solve a
  language membership problem you need computational strategies and tactics and
  resources. Simply put, we can /model computation/ most generally in terms of
  machinery that can input a string, and output a \lsquo{}yes\rsquo or a \lsquo{}no\rsquo --- \lsquo{}in the
  language\rsquo, or \lsquo{}not\rsquo.\rdquo

  Ila said, \ldquo{}But not every problem has a yes-or-no answer!\rdquo and Abu agreed,
  offering \ldquo{}Like sorting, which I understand to be a typical problem for
  computers.\rdquo

  \ldquo{}Ah, my young friends,\rdquo Til chuckled. \ldquo{}It so happens you are right, but
  computer scientists are clever people, and they have figured out a way to
  model a very large number of problems *as* decision problems, or as a series
  of decision problems. Your very example of sorting, Abu, is one of the
  easiest.\rdquo

  \ldquo{}How so?\rdquo said Abu, exchanging a puzzled look with Ila.

  \ldquo{}Look at a simple example,\rdquo Til said. \ldquo{}Sorting =(13 2 26)= (or some other
  permutation) in ascending order is a matter of answering three (to five)
  yes-or-no questions: is 13 less than 2 (no, so swap them), is 2 less than 26
  (yes, so don\rsquo{}t swap them), and, is 13 less than 26 (yes, so leave them where
  they are as well). The result: =(2 13 26)=.\rdquo

#+BEGIN_SRC emacs-lisp :results raw
  (let* ((unsorted '(13 2 26))
         (a (nth 0 unsorted))
         (b (nth 1 unsorted))
         (c (nth 2 unsorted)))
    (if (< a b)
        (if (< a c)
            (if (< b c)
                (list a b c)
              (list a c b))
          (list c a b))
      (if (< b c)
          (if (< a c)
              (list b a c)
            (list b c a))
        (list c b a))))
#+END_SRC

: (2 13 26)

  Ila was still puzzled. \ldquo{}How does that relate to a set membership decision
  problem?\rdquo Abu grinned his big, I-think-I-know grin, and said: \ldquo{}Let me try to
  answer that.\rdquo Til said, \ldquo{}Go ahead!\rdquo as Ila clenched her teeth. She thought she
  knew how now too.

  \ldquo{}In the realm of numbers, I can take the /language/ 

: ["1" "2" "3" "4" "5" "6" ...]

  and split it up into subsets like so:

: Less-than-2: ["1"]
:
: Less-than-3: ["1" "2"]
:
: Less-than-4: ["1" "2" "3"]

  and so on, as many as I like. Then for the question, is =a= less than =b=,
  just ask is =a= in the subset =Less-than-b=?\rdquo

  Ila frowned. \ldquo{}But isn\rsquo{}t that a way, way inefficient way to compare two
  numbers?\rdquo Til said, \ldquo{}Yes, it is, but if we\rsquo{}re not concerned with efficiency,
  that approach certainly works.\rdquo

  \ldquo{}But consider a big advantage of treating numbers as strings of digits. As you
  know, when the numbers get big we need special procedures if we want to do
  arithmetic with them. Let\rsquo{}s lump the relational operations with the arithmetic
  ones, and ask, how would one answer a simple =a < b= question, given:\rdquo

#+BEGIN_SRC emacs-lisp
  (setq a-as-string "361070123498760381765950923497698325576139879587987251757151" 
        b-as-string "36107058266725245759262937693558834387849309867353286761847615132"
        is-a-less-than-b (if (< (length a-as-string) (length b-as-string))
                             "Yes, a < b."
                           "No, a is not < b."))
#+END_SRC

: Yes, a < b.

  \ldquo{}That\rsquo{}s easy! =b= is bigger, because it has more digits,\rdquo said Ila. \ldquo{}Right,\rdquo
  said Abu. \ldquo{}At least, as long as the first five digits of =b= are not zeros!\rdquo
  Ila nodded agreement, \ldquo{}And even if the strings were the same length, a
  digit-by-digit comparison would soon reveal the answer.\rdquo Abu quickly added,
  \ldquo{}So, banning leading zeros in these strings-of-digits, /lexicographical/
  ordering comes to mind as a convenient way to sort them, one that can answer
  all relative size questions. Am I right?\rdquo

  Til nodded while Ila thought, Of course you are, smarty pants, then said, \ldquo{}But
  why the jargony *lexicographical*? Isn\rsquo{}t there a better word than that?
  \lsquo{}Lexicon\rsquo is just another word for \lsquo{}dictionary\rsquo, so why not just say \lsquo{}sort in
  /dictionary/ order\rsquo{}?\rdquo

  Abu said, \ldquo{}You mean use a shorter word? I don\rsquo{}t remember where I saw
  lexicographical, and no, I don\rsquo{}t know if there\rsquo{}s an another, less jargony way
  to say what it means. Technically speaking, what *does* it mean, Til?\rdquo

  \ldquo{}You\rsquo{}re about to find out!\rdquo Til said, as he flashed them his mischievous
  smile.

                    -~-~-~-~-~-
*** ZCF 

   In normal usage, a language is something we use to communicate, in speaking
   or writing. In theoretical computer science, a language is no more and no
   less than some subset of a set of all strings over some alphabet. Related
   formal definitions follow:

#+begin_note
    An *alphabet* is any non-empty, finite set (typically abbreviated \Sigma).

    Not letters, *symbols* are what the members or elements of a generic
    alphabet are called.

    A *string* is a finite sequence of symbols from a given alphabet. These are
    usually written as symbols placed [[side-by-side without adornments][side-by-side without adornments]] of
    brackets or braces, commas or spaces --- so abab rather than {a, b, a, b} or
    [a b a b].
 
    The *length* of a string is the number of symbols contained in the string.
    \vert{}w\vert denotes the length of w, in another overloading of vertical bars. The
    *empty* string (abbreviated \lambda or \epsilon) is a string that has a length of zero.
   
    The process of appending the symbols of one string to the end of another
    string, in the same order, is called *concatenation*.
#+end_note

  There is an operation in almost all programming languages to perform string
  concatenation, e.g.:

#+BEGIN_SRC emacs-lisp :results output
  (print (concat "ABC" "XYZ"))
#+END_SRC

: "ABCXYZ"

#+begin_note
  A method of listing strings called *lexicographic ordering* differs from
  so-called *dictionary ordering* in one essential way. The former method sorts
  strings /first/ by increasing length (so shorter strings come before longer
  ones) and /then/ by the predefined (dictionary) order of the symbols as given
  in association with the strings\rsquo alphabet.
#+end_note

  For instance, in lexicographical ordering the string =baa= would come
  before =abab= because it is shorter by one symbol. In plain old dictionary
  ordering lengths are ignored, so the string =abab= would come before =baa=,
  because =a= comes before =b= in the alphabet.

  Why this length-considering ordering is preferred will become clear when the \star
  operation is discussed below.

  To reiterate, a language is a subset of a set of strings. But which ones?
  That\rsquo{}s where grammars come into play.

#+begin_note
   A *Phrase-Structure Grammar* (PSG) is a four-tuple:

   =[N T S P]= where

   - =N= is a set of *Nonterminals* (also called *Variables*)
   - =T= is a set of *Terminals* (=N= \cap =T= = \emptyset)
   - =S= is the *Start* Nonterminal (=S= \in =N=)
   - =P= is a finite set of *Productions* (AKA *Rules*), each one mapping a
     Nonterminal to a string of Nonterminals and Terminals.
#+end_note

   To start with something familiar, here is a sample PSG\rsquo{}s =[N T S P]= for a
   (super small) subset of the English language:

:  N = [SENTENCE NOUN-PHRASE VERB-PHRASE ARTICLE ADJECTIVE NOUN VERB ADVERB]
:
:  S = SENTENCE
:
:  T = [the hungry sleepy cat dog chases runs quickly slowly]

   In the rules for this PSG, note that the vertical bar (\vert) means OR, e.g., the
   NOUN rule produces either =cat= or =dog= (exclusive-OR):

   Notice too that the terminals are just concrete words, the Nonterminals more
   abstract word /types/ (e.g. NOUN) normally used to talk /about/ and classify
   words. For the English language (as for any natural language) these terminal
   words form the set of symbols comprising the alphabet in the sense defined
   above.

   | P = [ |             |   |                                     |
   |       | SENTENCE    | \rightarrow | NOUN-PHRASE VERB-PHRASE NOUN-PHRASE |
   |       | SENTENCE    | \rightarrow | NOUN-PHRASE VERB-PHRASE             |
   |       | NOUN-PHRASE | \rightarrow | ARTICLE ADJECTIVE NOUN              |
   |       | NOUN-PHRASE | \rightarrow | ARTICLE NOUN                        |
   |       | VERB-PHRASE | \rightarrow | VERB-PHRASE ADVERB                  |
   |       | VERB-PHRASE | \rightarrow | VERB                                |
   |       | ARTICLE     | \rightarrow | the \vert \lambda                             |
   |       | ADJECTIVE   | \rightarrow | hungry \vert sleepy                     |
   |       | NOUN        | \rightarrow | cat \vert dog                           |
   |       | VERB        | \rightarrow | chases \vert runs                       |
   |       | ADVERB      | \rightarrow | slowly \vert quickly                    |
   | ]     |             |   |                                     |

#+begin_note
  The process of producing a sequence of terminals from the Start Nonterminal by
  replacing Nonterminals one at a time by applying some rule is called
  *derivation*. We apply a single rule and *derive* one string =R= from another
  string =L= if the rule has =L= on the left of its arrow and =R= on the right
  thereof.
#+end_note
 
  Derivation is thus an iterative process, illustrated with two random
  components in the following sample PSG-implementing code. Here, productions
  are represented as an alist of symbols. For each alist choice-list, the =car=
  is the symbol to the left of the arrow of a production, and the =cdr= captures
  the symbols to the right of the arrow, abbreviated LHS for left-hand-side, and
  RHS for right-hand-side. The =productions= alist is reversed and stored as
  well. Which alist (forward or reverse) to use at any step is the first choice
  that is randomly decided. Nonterminals are unbound symbols. Terminals are
  bound symbols whose values are either =t= or a string that differs from the
  symbol\rsquo{}s string name (e.g., =es= abbreviating \ldquo{}empty string\rdquo). Which terminal
  to use when only terminals are options (e.g., in the =ARTICLE=, =ADJECTIVE=,
  =NOUN=, =VERB= and =ADVERB= rules) is the second choice that is randomly
  decided.

#+BEGIN_SRC emacs-lisp :results silent
  (setq the t es "" hungry t sleepy t cat t dog t 
        chases t runs t slowly t quickly t
        productions
        '((SENTENCE NOUN-PHRASE VERB-PHRASE NOUN-PHRASE)
          (SENTENCE NOUN-PHRASE VERB-PHRASE)
          (NOUN-PHRASE ARTICLE ADJECTIVE NOUN)
          (NOUN-PHRASE ARTICLE NOUN)
          (VERB-PHRASE VERB ADVERB)
          (VERB-PHRASE VERB)
          (ARTICLE the es)
          (ADJECTIVE hungry sleepy)
          (NOUN cat dog)
          (VERB chases runs)
          (ADVERB slowly quickly))
        reverse-productions (reverse productions))

  (defun is-terminal (sym)
    (and (symbolp sym) (boundp sym)))

  (defun nonterminals-remain (derivation)
    (not (every 'is-terminal derivation)))

  (defun derive (LHS)
    (let* ((rules (if (zerop (random 2)) productions reverse-productions))
           (RHS (cdr (assoc LHS rules))))
      (if (null RHS)
          (list LHS)
        (if (nonterminals-remain RHS)
            RHS
          (list (nth (random (length RHS)) RHS))))))

  (defun transform-terminal (terminal)
    (or (and (boundp terminal)
             (stringp (symbol-value terminal))
             (symbol-value terminal))
        (symbol-name terminal)))

  (defun find-derivation (start-symbol)
    (let ((derivation (list start-symbol)))
      (while (nonterminals-remain derivation)
        (setq derivation (apply 'append (mapcar 'derive derivation))))
      (mapconcat 'transform-terminal derivation " ")))
#+END_SRC 

   The following derivation would result if the sequence of calls to =random=
   returned =[1 1 0 0 0 1 1 1 1 1 1]=:

   | SENTENCE | \rightarrow | NOUN-PHRASE VERB-PHRASE  |
   |          | \rightarrow | ARTICLE NOUN VERB-PHRASE |
   |          | \rarr | ARTICLE NOUN VERB ADVERB |
   |          | \rightarrow | the NOUN VERB ADVERB     |
   |          | \rightarrow | the dog VERB ADVERB      |
   |          | \rightarrow | the dog runs ADVERB      |
   |          | \rarr | the dog runs quickly     |

#+BEGIN_SRC emacs-lisp
  (find-derivation 'SENTENCE)
#+END_SRC

: the dog runs quickly

   Using the above example as a guide, produce derivations for each of the
   following sentences, and verify it by giving the return sequence of calls to
   =random=.

:EXERCISE:
    the sleepy cat runs slowly
:END:

:EXERCISE:
    the hungry dog runs quickly
:END:

:EXERCISE:
    the hungry dog chases the sleepy cat
:END:

:EXERCISE:
   Combinatorially speaking, how many different sentences can be derived by
   repeated evaluations of =(find-derivation 'SENTENCE)=?
:END:

   With this simple grammar is there a derivation for the following sentence?

   =the hungry sleepy dog runs=

   The answer is no. Adjectives do not follow other adjectives with the simple
   rule that ADJECTIVE produces either one terminal adjective (hungry) or the
   other (sleepy). English allows multiple adjectives, but it needs a more
   sophisticated rule, a \ldquo{}loopy\rdquo rule, i.e., a /recursive/ rule:
 
   ADJECTIVE \rightarrow ADJECTIVE ADJECTIVE \vert \lambda

   Moving towards more sophistication, what rules would you need to change or
   add to generate this sentence?

   =the quick brown fox jumps over the lazy dog=

   The answer is to add all of these except the first --- use it to replace the
   non-recursive ADJECTIVE rule:

   ADJECTIVE \rightarrow hungry \vert sleepy \vert quick \vert brown \vert lazy

   PREPOSITION \rightarrow of \vert from \vert by \vert on \vert in \vert over \vert \dots

   PREPOSITIONAL-PHRASE \rightarrow PREPOSITION NOUN-PHRASE

   VERB-PHRASE \rightarrow VERB PREPOSITIONAL-PHRASE

#+begin_note
   The derivation of a syntactically valid structured phrase from the top down
   can be visualized as the reverse of the process of building, from the bottom
   up, i.e., from leaves to root, a *syntax tree* (AKA a *parse tree*).
#+end_note

   For example, a valid sentence forms the leaves:

#+BEGIN_SRC dot :file img/fig-leaves-of-the-parse-tree.png :results output :exports results :eval no-export
  graph leaves {
    size="6,3";

    node [shape=none label=""] pa;
    node [shape=ellipse];

    a [label=the];
    b [label=hungry];
    c [label=dog];
    d [label=chases];
    e [label=the];
    f [label=sleepy];
    g [label=cat];

    pa -- a [style=invis];
    a -- b [style=invis];
    b -- c [style=invis];
    c -- d [style=invis];
    d -- e [style=invis];
    e -- f [style=invis];
    f -- g [style=invis];
    { rank=same; a b c d e f g }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-leaves-of-the-parse-tree.png]]

    Each terminal is given a parent, and as an alternate representation, each
    parent-child is rendered in tree form as a two-element list. Abbreviating
    ADJECTIVE as ADJ, ADVERB as ADV, ARTICLE as ART, NOUN as N, and VERB as V:

#+BEGIN_SRC dot :file img/fig-leaves-and-parents-of-the-parse-tree.png :exports results :eval no-export
  graph leavesandparents {
    size="6,3";

    node [shape=box];

    pa [label=ART];
    pb [label=ADJ];
    pc [label=N];
    pd [label=V];
    pe [label=ART];
    pf [label=ADJ];
    pg [label=N];

    a [shape=ellipse label=the];
    b [shape=ellipse label=hungry];
    c [shape=ellipse label=dog];
    d [shape=ellipse label=chases];
    e [shape=ellipse label=the];
    f [shape=ellipse label=sleepy];
    g [shape=ellipse label=cat];

    pa -- a;
    pb -- b;
    pc -- c;
    pd -- d;
    pe -- e;
    pf -- f;
    pg -- g;

    a -- b [style=invis];
    b -- c [style=invis];
    c -- d [style=invis];
    d -- e [style=invis];
    e -- f [style=invis];
    f -- g [style=invis];
    { rank=same; pa pb pc pd pe pf pg }
    { rank=same; a b c d e f g }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-leaves-and-parents-of-the-parse-tree.png]]

:  ((ART the) (ADJ hungry) (N dog) (V chases) (ART the) (ADJ sleepy) (N cat))

   Continuing on up the tree, we are also building a nested alist (albeit with
   repeated keys). Abbreviating NOUN-PHRASE as NP, and VERB-PHRASE as VP:

#+BEGIN_SRC dot :file img/fig-almost-complete-parse-tree.png :exports results :eval no-export
  graph almostcomplete {
    size="6,4";

    node [shape=box];

    np1 [label="NOUN-PHRASE"];
    vp  [label="VERB-PHRASE"];
    np2 [label="NOUN-PHRASE"];

    pa [label=ART];
    pb [label=ADJ];
    pc [label=N];
    pd [label=V];
    pe [label=ART];
    pf [label=ADJ];
    pg [label=N];

    a [shape=ellipse label=the];
    b [shape=ellipse label=hungry];
    c [shape=ellipse label=dog];
    d [shape=ellipse label=chases];
    e [shape=ellipse label=the];
    f [shape=ellipse label=sleepy];
    g [shape=ellipse label=cat];

    np1 -- pa -- a;
    np1 -- pb -- b;
    np1 -- pc -- c; 
    vp -- pd -- d;
    np2 -- pe -- e;
    np2 -- pf -- f;
    np2 -- pg -- g; 

    a -- b [style=invis];
    b -- c [style=invis];
    c -- d [style=invis];
    d -- e [style=invis];
    e -- f [style=invis];
    f -- g [style=invis];
    { rank=same; np1 vp np2 }
    { rank=same; pa pb pc pd pe pf pg }
    { rank=same; a b c d e f g }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-almost-complete-parse-tree.png]]

:  ((NP (ART the) (ADJ hungry) (N dog)) (VP (V chases)) (NP (ART the) (ADJ sleepy) (N cat)))

   Finishing with SENTENCE (abbrev. S) being the root of the tree, with the list
   form consing S on the front:

#+BEGIN_SRC dot :file img/fig-complete-parse-tree.png :exports results :eval no-export
  graph complete {
    size="6,5";

    node [shape=box];

    S   [label="SENTENCE"];
    np1 [label="NOUN-PHRASE"];
    vp  [label="VERB-PHRASE"];
    np2 [label="NOUN-PHRASE"];

    pa [label=ART];
    pb [label=ADJ];
    pc [label=N];
    pd [label=V];
    pe [label=ART];
    pf [label=ADJ];
    pg [label=N];

    a [shape=ellipse label=the];
    b [shape=ellipse label=hungry];
    c [shape=ellipse label=dog];
    d [shape=ellipse label=chases];
    e [shape=ellipse label=the];
    f [shape=ellipse label=sleepy];
    g [shape=ellipse label=cat];

    S -- np1;
    S -- vp;
    S -- np2;
    
    np1 -- pa -- a;
    np1 -- pb -- b;
    np1 -- pc -- c; 
    vp -- pd -- d;
    np2 -- pe -- e;
    np2 -- pf -- f;
    np2 -- pg -- g; 

    a -- b [style=invis];
    b -- c [style=invis];
    c -- d [style=invis];
    d -- e [style=invis];
    e -- f [style=invis];
    f -- g [style=invis];
    { rank=same; np1 vp np2 }
    { rank=same; pa pb pc pd pe pf pg }
    { rank=same; a b c d e f g }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-complete-parse-tree.png]]

:  (S (NP (ART the) (ADJ hungry) (N dog)) (VP (V chases)) (NP (ART the) (ADJ sleepy) (N cat)))

   See http://www.ironcreek.net/phpsyntaxtree/ for help with this process. (To
   use this tool, you must change the list-tree representation into nested
   vectors, rendering the phrase into what they call labelled bracket notation.)

#+BEGIN_SRC emacs-lisp
  (setq parsed [S [NP [ART the] [ADJ hungry] [N dog]] [VP [V
        chases]] [NP [ART the] [ADJ sleepy] [N cat]]])
  (kill-new (format "%s" parsed))
#+END_SRC

  [[file:simple-phrase.png][Visualize the hungry dog chasing the sleepy cat]].

:EXERCISE:
   Using the syntax-tree tool, build a parse tree for =the quick brown fox jumps
   over the lazy dog=.
:END:

:ANSWER:
#+BEGIN_SRC emacs-lisp
  (setq parsed [S [NP [ART the] [ADJ [ADJ quick] [ADJ brown]] [N
        fox]] [VP [V jumps] [PP [P over] [NP [ART the] [ADJ lazy]
        [N dog]]]]])
  (kill-new (format "%s" parsed))
#+END_SRC
:END:

   Build parse trees for the valid phrases:

     :EXERCISE:
     the cat runs
     :END:

     :EXERCISE:
     the cat chases the hungry dog
     :END:

     :EXERCISE:
     the dog runs quickly
     :END:

     :EXERCISE:
     the sleepy dog chases quickly the hungry cat
     :END:

  Let us examine a simple grammar in the programming languages direction, to
  produce, say, well-formed S-expressions. The basis for this grammar is a
  /skeleton/ for matching opening and closing parentheses, which has a recursive
  rule for enclosing in parentheses, and one for expanding the length of the
  string (plus a third for terminating the recursion):

  1. SKEL \rarr OP SKEL CP
  2. SKEL \rarr SKEL SKEL
  3. SKEL \rarr \lambda
  4. OP \rarr (
  5. CP \rarr )

  | SKEL | \rarr | SKEL SKEL             |
  |      | \rarr | OP SKEL CP SKEL       |
  |      | \rarr | OP OP SKEL CP CP SKEL |
  |      | \rarr | OP OP \lambda CP CP SKEL    |
  |      | \rarr | OP OP \lambda CP CP \lambda       |
  |      | \rarr | ( OP \lambda CP CP \lambda        |
  |      | \rarr | ( ( \lambda CP CP \lambda         |
  |      | \rarr | ( ( \lambda ) CP \lambda          |
  |      | \rarr | ( ( \lambda ) ) \lambda           |
  |      | \rarr | ( ( ) )               |

  But now a simple binary choice between forward and reverse productions fails,
  because of the /three/ possible derivations for SKEL.

#+BEGIN_SRC emacs-lisp :results silent
  (setq es "" open "(" close ")"
        productions
        '((SKEL OP SKEL CP)
          (SKEL es)
          (OP open)
          (CP close))
        reverse-productions (reverse productions))
#+END_SRC

#+BEGIN_SRC emacs-lisp
  (problem find-derivation 'SKEL) 
#+END_SRC

#+RESULTS:

:EXERCISE:
 Solve the problem in the code with finding a derivation starting with SKEL.
:END:

*** TODO Save a Harder Challenge for DM2                           :noexport:

  Go back to the original Grammar.

  Replace these three rules:

  ADJECTIVE \rightarrow Buffalo

  NOUN \rightarrow buffalo

  VERB \rightarrow buffalo

  With these new rules, is there a derivation for this "sentence"?!

**** This is a sentence?!
     Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo

**** Meaning Explained
     (The) Buffalo buffalo (that) Buffalo buffalo (often) buffalo (in turn)
     buffalo (other) Buffalo buffalo.

*** TODO Save Fancy Nouns for DM2                                  :noexport:

  Fancy nouns are *nested* nouns, for example "the fresh brownies that the
  little rascals without permission devoured" --- which could be rephrased as
  "the little rascals without permission devoured the fresh brownies, and it\rsquo{}s
  these brownies I want to focus on."

  So, a nested noun is a nested noun followed by a relative pronoun (e.g.,
  /that/) followed by a verb followed by a nested noun,

  OR,

  it\rsquo{}s a nested noun followed by a relative pronoun followed by a nested noun
  followed by a verb,

  OR,

  it\rsquo{}s a nested noun followed by a preposition followed by a nested noun,

  OR,

  it\rsquo{}s just an article followed by any number of adjectives followed by a plain
  old (non-nested) noun!

*** TODO Save Nested Nouns for DM2                                 :noexport:
 
    NESTED-NOUN \rightarrow NESTED-NOUN RELATIVE-PRONOUN VERB NESTED-NOUN

    NESTED-NOUN \rightarrow NESTED-NOUN RELATIVE-PRONOUN NESTED-NOUN VERB

    NESTED-NOUN \rightarrow PREPOSITION NESTED-NOUN

    NESTED-NOUN \rightarrow ARTICLE NOUN-PHRASE

    NOUN-PHRASE \rightarrow ADJECTIVE NOUN-PHRASE

    NOUN-PHRASE \rightarrow NESTED-NOUN

    NOUN-PHRASE \rightarrow NOUN
 
    ARTICLE \rightarrow a \vert an \vert the \vert \lambda

    RELATIVE-PRONOUN \rightarrow that \vert \lambda

    PREPOSITION \rightarrow of \vert from \vert by \vert \dots

**** Now It\rsquo{}s Possible

     Let NN = NESTED-NOUN, RP = RELATIVE-PRONOUN, es = \lambda (the empty string).

#+BEGIN_SRC emacs-lisp
  (setq parsed [S [NP [NN [NN [ART es] [NP [ADJ Buffalo] [NP [N
        buffalo]]]] [RP es] [NN [NP [ADJ Buffalo] [NP [N buffalo]]]][V
        buffalo]]] [VP [V buffalo]] [NP [ADJ Buffalo] [NP [N buffalo]]]])

  (kill-new (format "%s" parsed))
#+END_SRC


#+begin_note
  The grammar for [[English was long thought to be][English was long thought to be]] *context free*. The simple
  subset-of-English grammar we have been exploring is certainly not constrained
  by context. By way of contrast, here\rsquo{}s an example of two productions in a
  NON-context-free grammar:

  aAc \rightarrow aabc

  aAd \rightarrow abad

  Note that A\rsquo{}s expansion is different when it\rsquo{}s surrounded by a and c than when
  it\rsquo{}s surrounded by a and d. That means A\rsquo{}s expansion has context
  \ldquo{}sensitivity\rdquo. A grammar/language with this feature is called *context
  sensitive*.

  Moving down to the simplest type, a language is *regular* if it can be
  generated from its alphabet using the three *regular operations*:

  1. \cup (union)
  2. $\circ$ (concatenation)
  3. \star ([[Kleene star][Kleene star]] or just star)
#+end_note

  How these work can be crudely illustrated using a type of graph (or
  /pseudograph/, as loops are allowed) that could be taken for a \ldquo{}weighted\rdquo
  (actually just link-labeled) *directed* graph.

  - Union :: 0 \cup 1 --- we make a node with a link to another node for each
             /disjunct/ (0 or 1) --- so either path may be taken from the
             leftmost node.

#+BEGIN_SRC dot :file img/fig-three-state-union.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="4,3";

    node [shape=circle];

    Q0 [label=""];
    Q1 [label=""];
    Q2 [label=""];

    Q0 -> Q1 [label=0];
    Q0 -> Q2 [label=1];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-three-state-union.png]]

  - Concatenation :: 0 $\circ{}$ 1 (or just 01) --- we make a starting node and two
                     other nodes (one for each symbol) and a link for each
                     /conjunct/ (0 and 1) /in sequence/:

#+BEGIN_SRC dot :file img/fig-two-state-concatenation.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="4,1";

    node [shape=circle]; 
    Q0 [label=""];
    Q1 [label=""];
    Q2 [label=""];

    Q0 -> Q1 [label=0];
    Q1 -> Q2 [label=1];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-two-state-concatenation.png]]

  - Star :: 0^{\star} --- we make a node with a loop-link labeled with the symbol
            being \ldquo{}starred\rdquo.

#+BEGIN_SRC dot :file img/fig-one-state-star.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="2,2";

    node [shape=circle];
    Q0 [label=""]

    Q0 -> Q0 [label=0];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-one-state-star.png]]

  As how these separate operations compose into one graph can get somewhat
  complicated, we will forgo a complete description of the procedures and rules,
  but for the record, the most important rule for these graph compositions is:

#+begin_important
  /Every node must have one outgoing link for each symbol in the alphabet./
#+end_important

  A complicating feature of these operators is that they can be applied to more
  than just one symbol, e.g., (0 \cup 1)^{\star}, which means any number of
  repetitions (including zero) of a 0 or a 1, mixed and matched arbitrarily,
  which --- [[take this on faith if necessary][take this on faith if necessary]] --- eventually yields all possible
  strings over the alphabet =[0 1]= (AKA *bitstrings*).

  Mitigating some of the complexity, nodes can be split or merged (shared) and
  thus yield a simpler graph modeling the same language. For example, this graph
  models the language of all bitstrings that end in 1; equivalently, the regular
  language (0 \cup 1)^{\star}1:

#+BEGIN_SRC dot :file img/fig-bitstrings-ending-in-one.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="3,2";

    node [shape=circle];
    Q0 [label=""];
    Q1 [label=""];

    Q0 -> Q0 [label=0];
    Q0 -> Q1 [label=1];
    Q1 -> Q1 [label=1];
    Q1 -> Q0 [label=0];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-bitstrings-ending-in-one.png]]

  The node corresponding to the \star has been split in two, one loop labeled 0 and
  the other labeled 1, while the link for the 0 in the \cup is shared with the
  loop-link for the \star. Tracing different paths starting from the leftmost node
  and ending at the rightmost node, traversing links more than once being
  allowed, it is possible to generate the entire language --- every bitstring
  that ends in 1. The method is to concatenate onto a growing string (initially
  empty) a 0 as a link labeled 0 is traversed, and a 1 as a link labeled 1 is
  traversed. So, labeling the two nodes =L= and =R=, the path =L-R= yields =1=,
  the path =L-R-L-R= yields =101=, the path =L-R-L-L-R-R= yields =10011=, and so
  forth.

  Regular grammars generate regular languages, and so are amenable to this kind
  of graph modeling. In this representation of grammar as graph, nodes
  correspond to the Nonterminals, and links between nodes are the terminals,
  similar to how links and nodes worked in the above examples. But now let\rsquo{}s
  shift our focus and change the way we traverse these graphs. Instead of
  tracing paths to see what strings can be generated, we present some string to
  the graph and ask the *string* to try to trace its way through the graph.

  In so doing, we turn our graphs into machines, called [[state machines][state machines]]. The
  static picture of the graph is called a [[state diagram][state diagram]], the nodes are called
  states and the links are called transitions. So let\rsquo{}s revisit what happens
  with these machines from our new point of view, as we take on the persona of
  the machine (not the string, implied at the end of the previous paragraph to
  be the focus).

  By convention, the [[start node or state][start node or state]] (corresponding to the grammar\rsquo{}s Start
  symbol) is the node named =S= or with some symbol followed by one or more
  trailing 0s (e.g., s0, s00, etc.).
 
#+BEGIN_SRC dot :file img/fig-start-state.png :exports results :eval no-export
  digraph {
    size="1,1";

    node [shape=circle]; S;
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-start-state.png]]

  When we (the machine) are \ldquo{}turned on\rdquo we come up \ldquo{}in\rdquo our start state. We then
  start reading symbols, one by one, from our \ldquo{}input\rdquo --- the string being
  presented for processing --- and we move to other states according as the
  current symbol directs us to make one transition or another (the one labeled
  with that symbol). We do two things when making a transition:

  1. Update our current state to be the one at the other end of the transition
     link.
  2. \ldquo{}Consume\rdquo our currently read symbol and turn our attention to the next
     symbol in the input.

  Sample state transition on a 0:

#+BEGIN_SRC dot :file img/fig-state-transition-on-a-0.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="4,2";

    node [shape=circle];
    S1-> S2 [label="0"];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-state-transition-on-a-0.png]]

  Sample state transition on both 0 and 1 inputs. This is shorthand notation for
  two transitions, one for 0 and the other for 1:

#+BEGIN_SRC dot :file img/fig-state-transition-on-both-0-and-1.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="4,2";

    node [shape=circle];
    S1 -> S2 [label="0,1"];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-state-transition-on-both-0-and-1.png]]

  A double circle identifies an \ldquo{}accept\rdquo state. There can be more than one of
  these:

#+BEGIN_SRC dot :file img/fig-final-state.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="2,2";

    node [shape=doublecircle]; S4;
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-final-state.png]]

#+begin_note
  Shedding our machine persona, we look at it once again as an actor, or an
  agent making decisions. Accept states serve as language membership detectors.
  If a candidate input string is exhausted (entirely consumed by the transitions
  from state to state) at the exact transition an accept state is reached, the
  string is accepted as part of the language. A string exhausted in a
  non-accepting state is rejected --- it is *not* part of the language. If a
  machine accepts *all* strings that belong to a specific language, and rejects
  *all* those that do *not* belong to the language, then the machine is said to
  *recognize* the language.
#+end_note

  For example, the machine below recognizes the language =[01 011 0111]=:
 
#+BEGIN_SRC dot :file img/fig-three-string-language.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="5,1";

    node [shape=circle]; S A;
    node [shape=doublecircle]; B C D;

    S -> A [label=0];
    A -> B [label=1];
    B -> C [label=1];
    C -> D [label=1];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-three-string-language.png]]

  More correctly (remembering our [[important rule][important rule]] above), there should be
  transitions on each input character out of each state, thus:

#+BEGIN_SRC dot :file img/fig-three-string-language-complete.png :exports results :eval no-export
  digraph {
    size="5,3";

    node [shape=circle]; S A R;
    node [shape=doublecircle]; B C D;

    S -> A [label=0];
    A -> B [label=1];
    B -> C [label=1];
    C -> D [label=1];
    S -> R [label=1];
    A -> R [label=0];
    B -> R [label=0];
    C -> R [label=0];
    D -> R [label="0,1"];
    R -> R [label="0,1"];
    { rank=same; S A B C D }
    { rank=same; R }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-three-string-language-complete.png]]

   This machine realizes the 7-rule PSG below (only P is given, the other three
   components are inferable):

   1. S \rarr 0A
   2. A \rarr 1B
   3. B \rarr 1C
   4. B \rarr \lambda
   5. C \rarr 1D
   6. C \rarr \lambda
   7. D \rarr \lambda

   Recursive rules create loops, just like the \star operation. For example, the
   rule:

   A \rarr 0A

   yields

#+BEGIN_SRC dot :file img/fig-A-yields-0A.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="2,2";

    node [shape=circle];

    A -> A [label=0];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-A-yields-0A.png]]

   At node A, leave on a 0 and go back to A, as if the 0 in the rule were pulled
   to the left to label the arrow, and the A on the right were moved over and
   merged with the A on the left.

   For another example, the grammar:

   1. S \rarr 1A
   2. A \rarr 0A
   3. A \rarr 1A
   4. A \rarr \lambda

   is represented thus:

#+BEGIN_SRC dot :file img/fig-bitstrings-starting-with-one.png :exports results :eval no-export
  digraph {

    rankdir="LR";
    size="2,2";

    node [shape=circle]; S;
    node [shape=doublecircle]; A;

    S -> A [label=1];
    A -> A [label="0,1"];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-bitstrings-starting-with-one.png]]

   This is almost like the (0 \cup 1)^{\star}1 machine, except it recognizes all
   bitstrings that /start/ with a one: 1(0 \cup 1)^{\star}

:EXERCISE:
  Add a state and the necessary transitions to make this a valid state machine.
:END:

:EXERCISE:
  Compare/contrast this machine with the one for (0 \cup 1)^{\star}1 shown above. 
:END:

   This machine recognizes the language of all bitstrings whose /second-to-last/
   bit is a 0:

#+BEGIN_SRC dot :file img/fig-second-to-last-is-0.png :exports results :eval no-export
  digraph {
    size="3,5";

    node [shape=circle]; S0 S1;
    node [shape=doublecircle]; S2 S3;

    S0 -> S0 [label=1 headport=n tailport=n];
    S0 -> S1 [label=0];
    S1 -> S2 [label=1];
    S1 -> S3 [label=0];
    S2 -> S0 [label=1];
    S2 -> S1 [label=0];
    S3 -> S2 [label=1];
    S3 -> S3 [label=0 headport=s tailport=s];
    { rank=same; S0 S1 }
    { rank=same; S2 S3 }
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-second-to-last-is-0.png]]

:EXERCISE:
  The three-production grammar

  1. S \rarr A1
  2. A \rarr A0
  3. A \rarr \lambda
  
  generates the simple language consisting of any number of 0\rsquo{}s (including zero
  0\rsquo{}s) followed by a single 1.

  Build a simple two-state machine to model it. 

# ANSWER
#     _0_
#    /   \
#    \   /
#     v /
#     (S)---1--->((F))
:END:

:EXERCISE:
  The following machine recognizes which language?

#+BEGIN_SRC dot :file img/fig-single-state-with-01-transitions.png :exports results :eval no-export
  digraph {
    size="1,2";

    node [shape=circle]; S;

    S -> S [label=0 headport=n tailport=n];
    S -> S [label=1 headport=s tailport=s];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-single-state-with-01-transitions.png]]

# ANSWER
# The empty language \emptyset. With no accept states, no string can be accepted --- all
# are rejected.
:END:

  The language recognizers we have been examining are not the only
  interpretation we can give these kinds of state machines. You may have seen
  others in a course on digital logic and circuits. Here follows one of the
  simplest possible examples.

  Consider a 1-bit computer controlling some lights in a room equipped with
  motion sensors. The lights are either off or on. The state of the lights can
  thus be remembered with just 1 bit of memory --- 0 for off and 1 for on. The
  lights are controlled --- toggled on and off --- based on motion (or lack
  thereof) detected by the motion sensors, which are also connected to a timer.

  The lights are initially off, so the computer starts in the OFF state. In this
  state, only the MOTION input causes it to move to the ON state, which causes
  the lights to go on. In the ON state, a MOTION input causes it to remain in
  the ON state (the lights stay on), and also resets the no-motion timer. With
  the timer reset, after a certain time elapses (with no further MOTION inputs)
  the input NO-MOTION is triggered. This input causes it to move to the OFF
  state, which turns the lights off.

#+BEGIN_SRC dot :file img/fig-one-bit-computer.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="5,3";

    node [shape=circle];

    S0 [label=OFF];
    S1 [label=ON];

    S0 -> S0 [label="NO-MOTION"];
    S0 -> S1 [label="MOTION"];
    S1 -> S1 [label="MOTION"];
    S1 -> S0 [label="NO-MOTION"];
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-one-bit-computer.png]]

  What language is generated by a given grammar?

  Let V = [S A B] and T = [0 1]. Find the language generated by each grammar

  [V T S P]

  when the set P of productions consists of each of the following:

:EXERCISE:
    S \rightarrow AB

    A \rightarrow 01

    B \rightarrow 11
:END:

:EXERCISE:
    S \rightarrow AB

    S \rightarrow 0A

    A \rightarrow 0

    B \rightarrow 10
:END:

:EXERCISE:
    S \rightarrow AB

    S \rightarrow AA

    A \rightarrow 0B

    A \rightarrow 01

    B \rightarrow 1
:END:

:EXERCISE:
    S \rightarrow AA

    S \rightarrow B

    A \rightarrow 00A

    A \rightarrow 00

    B \rightarrow 1B

    B \rightarrow 1
:END:

:EXERCISE:
    S \rightarrow AB

    A \rightarrow 0A1

    B \rightarrow 1B0

    A \rightarrow \lambda

    B \rightarrow \lambda
:END:

#+BEGIN_SRC emacs-lisp :results silent
  (setq es "" a "0" b "1"
        productions
        '((S A B)
          (A a A b)
          (B b B a)
          (A es)
          (B es))
        reverse-productions (reverse productions))
#+END_SRC

#+BEGIN_SRC emacs-lisp
  (find-derivation 'S)
#+END_SRC

: 0 0  1 1 1 1  0 0

  What grammar generates a given language?

  :EXERCISE:
  Construct a PSG to generate {0^{2n }1 \vert n \ge 0}.
  :END:

  :EXERCISE:
  Construct a PSG to generate {0^{n }1^{2n} \vert n \ge 0}.
  :END:

  :EXERCISE:
  Construct a PSG to generate {0^n 1^m 0^n \vert m \ge 0 and n \ge 0}.
  :END:

*** ILO  

   Noam Chomsky is a linguist who first proposed the hierarchical language
   classification scheme that now bears his name.

   - The Chomsky Hierarchy in the Universal Set of All Languages (the superset of Types 0 through 3) ::
#+BEGIN_SRC ditaa :file img/fig-chomsky-hierarchy.png :exports results :eval no-export
    /------------------------------------------------------\
    |   Type 0 Recursively Enumerable Languages            |
    |   /----------------------------------------------\   |
    |   |    Type 1 Context Sensitive Languages        |   |
    |   |    /-------------------------------------\   |   |
    |   |    |   Type 2 Context Free Languages     |   |   |
    |   |    |   /-----------------------------\   |   |   |
    |   |    |   |  Type 3 Regular Languages   |   |   |   |
    |   |    |   |                             |   |   |   |
    |   |    |   |                             |   |   |   |
    |   |    |   \-----------------------------/   |   |   |
    |   |    |                                     |   |   |
    |   |    \-------------------------------------/   |   |
    |   |                                              |   |
    |   \----------------------------------------------/   |
    |                                                      |
    \------------------------------------------------------/
#+END_SRC

#+RESULTS:
[[file:img/fig-chomsky-hierarchy.png]]

   - A Tabular Taxonomy :: The following table maps the notions of language
        classes with the types of grammars that can generate those languages.
        The restrictions on productions distinguish what\rsquo{}s what (where N =
        Nonterminal, tl = terminal, LHS = Left-Hand Side, RHS = Right-Hand
        Side).

   | Language Class         | Type | Restrictions on Grammar Productions       |
   |------------------------+------+-------------------------------------------|
   | Recursively Enumerable |    0 | No restrictions                           |
   |                        |      | (length of LHS may exceed length of RHS). |
   |                        |      |                                           |
   | Context Sensitive      |    1 | LHS may have more than one Nonterminal,   |
   |                        |      | but the length of the LHS must be         |
   |                        |      | at most the length of the RHS             |
   |                        |      | (except for S \rarr \lambda productions).           |
   |                        |      |                                           |
   | Context Free           |    2 | Each LHS must have only one Nonterminal.  |
   |                        |      |                                           |
   | Regular                |    3 | Left-linear or Right-linear               |
   |                        |      | (each RHS must be either a tl or \lambda,       |
   |                        |      | or have a single Nonterminal and be       |
   |                        |      | all like Ntl, or all like tlN).           |

**** TODO Save These Classification Exercises for DM2              :noexport:

   In the following exercises you must classify/distinguish the four grammar
   types.

   Let N = [S A B], T = [a b], and G = [N T S P] (P to be given later).

   Determine whether G

   - is a type 0 grammar but not a type 1 grammar, or
   - is a type 1 grammar but not a type 2 grammar, or
   - is a type 2 grammar but not a type 3 grammar, or 
   - is a type 3 grammar,

   when P, the set of productions, is one of the following:

:EXERCISE:
    S \rightarrow 0AB 

    A \rightarrow B1

    B \rightarrow \lambda
:END:

:ANSWER:
  2 but not 3. The first production disqualifies it from being 3, being
  neither Left-linear nor Right-linear.
:END:

:EXERCISE:
    S \rightarrow 0A

    A \rightarrow 1B

    B \rightarrow 1
:END:

:ANSWER:
  This is a type 3, Right-linear grammar.
:END:

:EXERCISE:
    S \rightarrow AB0

    AB \rightarrow 10
:END:

:ANSWER:
  Type 1 but not type 2. Second production violates the Type 2 restriction.
:END:

:EXERCISE:
    S \rightarrow BAB

    A \rightarrow 0A

    B \rightarrow 01
:END:

:ANSWER:
  2 but not 3. First production is neither Left- nor Right-linear.
:END:

:EXERCISE:
    S \rightarrow 0A

    0A \rightarrow B

    B \rightarrow 0A

    A \rightarrow 1
:END:

:ANSWER:
  0 but not 1. Second production disqualifies it from being 1.
:END:

:EXERCISE:
    S \rightarrow A0

    A \rightarrow 0

    S \rightarrow \lambda

:END:

:ANSWER:
  Type 3, Left-linear.
:END:

** TODO Save for DM2 this more detailed description/definition     :noexport:
   A language is /regular/ *iff* some /regular expression/ describes it.

   Regular expressions use the so-called regular operations (\cup, \circ, and \star) ---
   (union, concatenation, and star) --- to build regular languages. Here is a
   recursive definition:

  R is a *regular expression* (an *re* for short) if R is any of

  - \emptyset
  - {\lambda}
  - {a} for some a \in \Sigma
  - R_1 \cup R_2, where R_1 and R_2 are *re*\rsquo{}s
  - R_1 \circ R_2, where R_1 and R_2 are *re*\rsquo{}s
  - R^{\star}, where R is an *re*
 
  Some shorthand:

  - a \equiv \{a\}
  - \lambda \equiv \{\lambda\}
  - R^{\plus} \equiv R \circ R^{\star}
  - R^{\star} \equiv R^{\plus} \cup \lambda
  - R^k \equiv R \circ R \circ R \circ \dots \circ R (k times)

  Note: R \circ R is usually written without the \circ, i.e., RR. In this way \circ is
  analogous to the multiplication operator.

** TODO Give some examples of *re*'s                               :noexport:
   Like 01^{\star} ; 0(0 \cup 1)^{\star} 

** TODO Develop Ideas For an Epilogue, or Other Episodic Installments

  Solving a math problem to get clues to Til\rsquo{}s unknown whereabouts would be a
  fitting conclusion, while also foreshadowing further interaction and learning
  adventures of Til, Ila and Abu.

  Til has gone missing for two weeks. He knows where he is, but has no way to
  communicate his location in the desert where he went to seek solitude.
  Something he feels compelled to do from time to time, much to his wife\rsquo{}s
  dismay. This time he is in some kind of trouble, trapped without means to get
  out on his own.

  The problem is that his tracer signal is encrypted, in a very eccentric way.
  This way may have something to do with the final puzzle he gave Abu and Ila,
  namely, to find the connection between Edgar Allan Poe and the phrase \ldquo{}Notice
  cousin Felipe\rdquo.

* )

  <<a poem by Wordsworth>>
  - a poem by Wordsworth :: [[https://www.poetryfoundation.org/poems-and-poets/poems/detail/45518][The French Revolution as It Appeared to Enthusiasts at Its Commencement]]

  <<think about language>>
  - think about language :: As reported in the book /Incompleteness: The Proof
       and Paradox of Kurt G\ouml{}del/, by Rebecca Goldstein, page 110. See also page
       112, at the top. See link to book below.

  <<Math writing is notorious>>
  - Math writing is notorious :: Adapted from a document on [[http://jmlr.csail.mit.edu/reviewing-papers/knuth_mathematical_writing.pdf][Mathematical Writing]]
       by Donald E. Knuth, Tracy Larrabee, and Paul M. Roberts, based on a course
       given at Stanford University, here\rsquo{}s an example of bad math writing:

       If L^{+}(P, N_{0}) is the set of functions f : P \rarr N_{0} with the property that

       \exists_{n_0 \in N_0 } \forall_{p \in P } p \ge n_{0} \rArr f(p) = 0

       then there exists a bijection N_1 \rarr L^{+}(P, N_{0}) such that if n \rarr f then  

       n = \prod_{p \in P} p^{f(p)}.

       Here P is the prime numbers, and N_{1} = N_{0} \setminus {0}.

       Here is Donald Knuth, writing with *clarity* what the above is trying to
       say:

       According to the Fundamental Theorem of Arithmetic, each positive integer
       u can be expressed in the form 

       u = 2^a 3^b 5^c 7^d 11^e \dots

       where *all* primes in increasing order are in the product, where the
       exponents a, b, c, d, e, \dots are uniquely determined nonnegative integers,
       and where all but a finite number of the exponents are zero.

  <<foreign language of mathematical jargon>>
  - foreign language of mathematical jargon :: From the book /Coincidences,
       Chaos, and All That Math Jazz --- Making Light of Weighty Ideas/ by Edward
       B. Burger and Michael Starbird, page viii:
       [[https://www.amazon.com/Coincidences-Chaos-That-Math-Jazz/dp/0393329313]]

  <<vocabulary and syntax of human language>>
  - vocabulary and syntax of human language :: From the book /Interactions: A/
       /Journey Through the Mind of a Particle Physicist and the Matter of This
       World/, by Sheldon L. Glashow, page 47:
       [[https://www.amazon.com/Interactions-Journey-Through-Particle-Physicist/dp/0446513156]]

  <<a description of formal systems>>
  - a description of formal systems :: From the book /Incompleteness: The Proof
       and Paradox of Kurt G\ouml{}del/, by Rebecca Goldstein, page 86:
       [[https://www.amazon.com/Incompleteness-Proof-Paradox-G%25C3%25B6del-Discoveries/dp/0393327604]]

  <<beads on a string>>
  - beads on a string :: Found in the book /The Stuff of Thought: Language as a
       Window into Human Nature/ by Steven Pinker:
       [[https://www.amazon.com/Stuff-Thought-Language-Window-Nature/dp/0143114247]]

       This analogy is also used (less successfully) to describe chromosomes as
       being made up of genes (the beads).

  <<simply trying to divide 23 by 2 and 3>>
  - simply trying to divide 23 by 2 and 3 :: Why do we not need to also do trial
       division of 23 by 5, 7, 11, etc., to clinch its primeness?

  <<Sieve of Eratosthenes>>
  - Sieve of Eratosthenes :: Using some elisp code to augment paper-and-pencil
       investigation:

#+BEGIN_SRC emacs-lisp
  (loop for n from 2 to 97 by 7
        collect (loop for i from 0 below 7
                      collect (+ i n)))
#+END_SRC

  |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
  |  9 | 10 | 11 | 12 | 13 | 14 | 15 |
  | 16 | 17 | 18 | 19 | 20 | 21 | 22 |
  | 23 | 24 | 25 | 26 | 27 | 28 | 29 |
  | 30 | 31 | 32 | 33 | 34 | 35 | 36 |
  | 37 | 38 | 39 | 40 | 41 | 42 | 43 |
  | 44 | 45 | 46 | 47 | 48 | 49 | 50 |
  | 51 | 52 | 53 | 54 | 55 | 56 | 57 |
  | 58 | 59 | 60 | 61 | 62 | 63 | 64 |
  | 65 | 66 | 67 | 68 | 69 | 70 | 71 |
  | 72 | 73 | 74 | 75 | 76 | 77 | 78 |
  | 79 | 80 | 81 | 82 | 83 | 84 | 85 |
  | 86 | 87 | 88 | 89 | 90 | 91 | 92 |
  | 93 | 94 | 95 | 96 | 97 | 98 | 99 |

  We take these integers and manually sieve them --- filtering out all nonprimes
  --- by crossing out every other number (after 2 --- so 4, 6, 8, etc. are
  crossed out), which excludes the multiples of 2, every third number (after 3),
  which drops the multiples of 3, every fifth number (after 5) to filter out the
  multiples of 5, etc. Note that some numbers (e.g., the multiples of 6) get
  crossed out twice --- once for the multiples-of-2 sieving, once for the
  multiples-of-3 sieving --- and this is an acceptable redundancy, as it avoids
  the continual use of a conditional that says only cross a number out if it is
  not already crossed out!

  |    2 |    3 |  +4+ |    5 |  +6+ |    7 |  +8+ |
  |    9 | +10+ |   11 | +12+ |   13 | +14+ |   15 |
  | +16+ |   17 | +18+ |   19 | +20+ |   21 | +22+ |
  |   23 | +24+ |   25 | +26+ |   27 | +28+ |   29 |
  | +30+ |   31 | +32+ |   33 | +34+ |   35 | +36+ |
  |   37 | +38+ |   39 | +40+ |   41 | +42+ |   43 |
  | +44+ |   45 | +46+ |   47 | +48+ |   49 | +50+ |
  |   51 | +52+ |   53 | +54+ |   55 | +56+ |   57 |
  | +58+ |   59 | +60+ |   61 | +62+ |   63 | +64+ |
  |   65 | +66+ |   67 | +68+ |   69 | +70+ |   71 |
  | +72+ |   73 | +74+ |   75 | +76+ |   77 | +78+ |
  |   79 | +80+ |   81 | +82+ |   83 | +84+ |   85 |
  | +86+ |   87 | +88+ |   89 | +90+ |   91 | +92+ |
  |   93 | +94+ |   95 | +96+ |   97 | +98+ |   99 |

  Now cross out the multiples of 3:

  |    2 |    3 |  +4+ |    5 |  +6+ |    7 |  +8+ |
  |  +9+ | +10+ |   11 | +12+ |   13 | +14+ | +15+ |
  | +16+ |   17 | +18+ |   19 | +20+ | +21+ | +22+ |
  |   23 | +24+ |   25 | +26+ | +27+ | +28+ |   29 |
  | +30+ |   31 | +32+ | +33+ | +34+ |   35 | +36+ |
  |   37 | +38+ | +39+ | +40+ |   41 | +42+ |   43 |
  | +44+ | +45+ | +46+ |   47 | +48+ |   49 | +50+ |
  | +51+ | +52+ |   53 | +54+ |   55 | +56+ | +57+ |
  | +58+ |   59 | +60+ |   61 | +62+ | +63+ | +64+ |
  |   65 | +66+ |   67 | +68+ | +69+ | +70+ |   71 |
  | +72+ |   73 | +74+ | +75+ | +76+ |   77 | +78+ |
  |   79 | +80+ | +81+ | +82+ |   83 | +84+ |   85 |
  | +86+ | +87+ | +88+ |   89 | +90+ |   91 | +92+ |
  | +93+ | +94+ |   95 | +96+ |   97 | +98+ | +99+ |

  Now cross out the multiples of 5:

  |    2 |    3 |  +4+ |    5 |  +6+ |    7 |  +8+ |
  |  +9+ | +10+ |   11 | +12+ |   13 | +14+ | +15+ |
  | +16+ |   17 | +18+ |   19 | +20+ | +21+ | +22+ |
  |   23 | +24+ | +25+ | +26+ | +27+ | +28+ |   29 |
  | +30+ |   31 | +32+ | +33+ | +34+ | +35+ | +36+ |
  |   37 | +38+ | +39+ | +40+ |   41 | +42+ |   43 |
  | +44+ | +45+ | +46+ |   47 | +48+ |   49 | +50+ |
  | +51+ | +52+ |   53 | +54+ | +55+ | +56+ | +57+ |
  | +58+ |   59 | +60+ |   61 | +62+ | +63+ | +64+ |
  | +65+ | +66+ |   67 | +68+ | +69+ | +70+ |   71 |
  | +72+ |   73 | +74+ | +75+ | +76+ |   77 | +78+ |
  |   79 | +80+ | +81+ | +82+ |   83 | +84+ | +85+ |
  | +86+ | +87+ | +88+ |   89 | +90+ |   91 | +92+ |
  | +93+ | +94+ | +95+ | +96+ |   97 | +98+ | +99+ |

  Now cross out the three remaining multiples of 7:

  |    2 |    3 |  +4+ |    5 |  +6+ |    7 |  +8+ |
  |  +9+ | +10+ |   11 | +12+ |   13 | +14+ | +15+ |
  | +16+ |   17 | +18+ |   19 | +20+ | +21+ | +22+ |
  |   23 | +24+ | +25+ | +26+ | +27+ | +28+ |   29 |
  | +30+ |   31 | +32+ | +33+ | +34+ | +35+ | +36+ |
  |   37 | +38+ | +39+ | +40+ |   41 | +42+ |   43 |
  | +44+ | +45+ | +46+ |   47 | +48+ | +49+ | +50+ |
  | +51+ | +52+ |   53 | +54+ | +55+ | +56+ | +57+ |
  | +58+ |   59 | +60+ |   61 | +62+ | +63+ | +64+ |
  | +65+ | +66+ |   67 | +68+ | +69+ | +70+ |   71 |
  | +72+ |   73 | +74+ | +75+ | +76+ | +77+ | +78+ |
  |   79 | +80+ | +81+ | +82+ |   83 | +84+ | +85+ |
  | +86+ | +87+ | +88+ |   89 | +90+ | +91+ | +92+ |
  | +93+ | +94+ | +95+ | +96+ |   97 | +98+ | +99+ |

  Now to do with code what we just did manually. We cross out a number by
  negating it (making it negative) and must use a conditional to avoid undoing
  that negation once done.

#+BEGIN_SRC emacs-lisp
  (require 'calc-misc) ;; for math-posp

  (defun negate-multiple (n m)
    (if (and (/= n m) (zerop (mod n m)))
        (if (< n 0) n (- n))
      n))

  (let* ((all (number-sequence 2 25))
         (all-minus-multiples-of-2
          (mapcar (lambda (n) (negate-multiple n 2))
                  all))
         (all-minus-multiples-of-2-and-3
          (mapcar (lambda (n) (negate-multiple n 3))
                  all-minus-multiples-of-2))
         (all-minus-multiples-of-2-and-3-and-5
          (mapcar (lambda (n) (negate-multiple n 5))
                  all-minus-multiples-of-2-and-3)))
    (list all-minus-multiples-of-2 
          all-minus-multiples-of-2-and-3
          all-minus-multiples-of-2-and-3-and-5
          (remove-if-not 'math-posp all-minus-multiples-of-2-and-3-and-5)))
#+END_SRC

| 2 | 3 | -4 | 5 | -6 |  7 | -8 |  9 | -10 | 11 | -12 | 13 | -14 |  15 | -16 | 17 | -18 | 19 | -20 |  21 | -22 | 23 | -24 |  25 |
| 2 | 3 | -4 | 5 | -6 |  7 | -8 | -9 | -10 | 11 | -12 | 13 | -14 | -15 | -16 | 17 | -18 | 19 | -20 | -21 | -22 | 23 | -24 |  25 |
| 2 | 3 | -4 | 5 | -6 |  7 | -8 | -9 | -10 | 11 | -12 | 13 | -14 | -15 | -16 | 17 | -18 | 19 | -20 | -21 | -22 | 23 | -24 | -25 |
| 2 | 3 |  5 | 7 | 11 | 13 | 17 | 19 |  23 |    |     |    |     |     |     |    |     |    |     |     |     |    |     |     |

  This final sieve operates stage by stage (recording a copy of each stage for
  later inspection) using a recursive deletion of nonprimes, starting with a
  complete number sequence from 2 to some limit. Not the most efficient sieve,
  but passable.

#+BEGIN_SRC emacs-lisp
  (require 'cl)

  (setq stages nil)

  (defun delete-nonprimes (a)
    (push (copy-sequence a) stages)
    (if (> (length a) 1)
        (delete-if (lambda (n) (zerop (mod n (car a)))) (cdr a)))
    (if (> (length a) 1)
        (delete-nonprimes (cdr a)))
    a)

  (defun sieve-of-Eratosthenes (limit)
    (delete-nonprimes (number-sequence 2 limit))) 
#+END_SRC

#+BEGIN_SRC emacs-lisp
  (sieve-of-Eratosthenes 99) 
#+END_SRC

 | 2 | 3 | 5 | 7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |

#+BEGIN_SRC emacs-lisp
  stages 
#+END_SRC

  | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  |  7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 49 | 53 | 59 | 61 | 67 | 71 | 73 | 77 | 79 | 83 | 89 | 91 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  |  5 |  7 | 11 | 13 | 17 | 19 | 23 | 25 | 29 | 31 | 35 | 37 | 41 | 43 | 47 | 49 | 53 | 55 | 59 | 61 | 65 | 67 | 71 | 73 | 77 | 79 | 83 | 85 | 89 | 91 | 95 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  |  3 |  5 |  7 |  9 | 11 | 13 | 15 | 17 | 19 | 21 | 23 | 25 | 27 | 29 | 31 | 33 | 35 | 37 | 39 | 41 | 43 | 45 | 47 | 49 | 51 | 53 | 55 | 57 | 59 | 61 | 63 | 65 | 67 | 69 | 71 | 73 | 75 | 77 | 79 | 81 | 83 | 85 | 87 | 89 | 91 | 93 | 95 | 97 | 99 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32 | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 | 41 | 42 | 43 | 44 | 45 | 46 | 47 | 48 | 49 | 50 | 51 | 52 | 53 | 54 | 55 | 56 | 57 | 58 | 59 | 60 | 61 | 62 | 63 | 64 | 65 | 66 | 67 | 68 | 69 | 70 | 71 | 72 | 73 | 74 | 75 | 76 | 77 | 78 | 79 | 80 | 81 | 82 | 83 | 84 | 85 | 86 | 87 | 88 | 89 | 90 | 91 | 92 | 93 | 94 | 95 | 96 | 97 | 98 | 99 |

  The results if the recording is simply =(push a stages)= shows how the
  destructive delete culls out nonprimes from the same =a= list on every stage:

  | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |    |
  | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |    |
  | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |    |
  | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |    |
  | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |    |
  | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |    |
  | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |    |
  | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |    |
  | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |    |
  | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |    |
  |  7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |    |
  |  5 |  7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |    |
  |  3 |  5 |  7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |    |
  |  2 |  3 |  5 |  7 | 11 | 13 | 17 | 19 | 23 | 29 | 31 | 37 | 41 | 43 | 47 | 53 | 59 | 61 | 67 | 71 | 73 | 79 | 83 | 89 | 97 |

  <<side-by-side without adornments>>
  - side-by-side without adornments :: They are thus like lisp symbols, whose
       names are lisp strings, which are sequences of characters, which are
       integers. They differ from lisp symbols by accommodating more alphabets.
       For example, =123= is a string over the alphabet =[0 1 2 3 4 5 6 7 8 9]=
       --- in lisp it would be a number.

  <<English was long thought to be>>
  - English was long thought to be :: Context Free.
       But see [[https://www.jstor.org/stable/4178381][English Is Not a Context-Free Language]], James Higginbotham,
       Linguistic Inquiry Vol. 15, No. 2 (Spring, 1984), pages 225-234.

  <<Kleene star>>
  - Kleene star :: Named after [[https://en.wikipedia.org/wiki/Stephen_Cole_Kleene][Stephen Cole Kleene]], who is the mathematician who
                   defined this powerful operation on either a set of symbols or
                   a set of strings. The article referenced defines V^{\star}
                   \ldquo{}as the set of finite-length strings that can be generated by
                   concatenating arbitrary elements of V, allowing the use of
                   the same element multiple times.\rdquo

  <<take this on faith if necessary>>
  - take this on faith if necessary :: For illustration\rsquo{}s sake, and analogous to
       the set of all bitstrings, take the alphabet =[a b]=, where the Kleene
       star produces the following set: =["" "a" "b" "aa" "ab" "ba" "bb" "aaa"
       "aab" "aba" "abb" "baa" "bab" "bba" "bbb" "aaaa" ...]=. Note that if we
       used standard dictionary order to list this set, it would start like =[""
       "a" "aa" "aaa" "aaaa" ...]= and never get to ="b"=, etc. That is why the
       length-considering lexicographic way is preferred.

  <<state machines>>
  - state machines :: Or *finite-state machines* or *finite-state automata*
                      (*automaton* singular)). 

  <<state diagram>>
  - state diagram :: Or *state-transition diagram*.

  <<important rule>>
  - important rule :: This rule must be adhered to for the machine to be a valid
                      *deterministic* finite automaton (DFA). The rule can be
                      relaxed when we are ready to explore the related
                      *nondeterministic* finite automata (NFA).

  <<start node or state>>
  - start node or state :: More conventionally, the start state is identified by
       an incoming arrow pointing to it (but coming from nowhere). Or, in
       another form of ornamentation, a triangle pointing to it is sometimes
       used to mark the start state.
 
#+BEGIN_SRC dot :file img/fig-start-state-with-incoming-arrow.png :exports results :eval no-export
  digraph {
    rankdir="LR";
    size="2,2";

    node [shape=none label=""]; start;
    node [shape=circle label=""]; S0;
    start -> S0;
  }
#+END_SRC

#+RESULTS:
[[file:img/fig-start-state-with-incoming-arrow.png]]
