#+TITLE: Last Chapter
#+LANGUAGE: en
#+OPTIONS: H:4 num:nil toc:nil \n:nil @:t ::t |:t ^:t *:t TeX:t LaTeX:t
#+STARTUP: showeverything entitiespretty

* TODO YZ@

  \ldquo{}Isn\rsquo{}t it interesting how languages either facilitate or impede
  communication?\rdquo asked Til, just as Ila and Abu entered his den. Ila replied,
  without missing a beat, \ldquo{}I know programming languages facilitate communication
  with computers, but I don\rsquo{}t see the impediment part\rdquo{}. \ldquo{}I do,\rdquo said Abu. Not
  knowing any programming languages, except a little lisp, I suppose, impedes my
  communication with you when you\rsquo{}re talking programming. Yes, but ignorance on
  your part is not the fault of the languages, said Ila. So, Til, did you mean
  ignorance of languages is the impediment?

  No, not necessarily. Sometimes a perfectly natural and known language like
  English is used, or I should say /misused/, to obfuscate rather than
  communicate --- to hide rather than reveal meaning. I\rsquo{}ll come back to this
  point later.

  So the question, what is a language, with its obvious answer, a language is
  something humans use to communicate, is too general in one sense, and too
  specific in another.

  As we\rsquo{}ll see, languages can be generalized to an arbitrary set of strings, but
  made more specific by /how/ that set is defined. Phrase-Structure Grammars ---
  PSGs --- are the tools (rules) used to define which strings are valid members
  of the language associated with that PSG. Now let me be clear what that
  association is. It is that PSGs /generate/ languages, and so of course,
  languages are /generated/ (built) by PSGs.

  In English (or any natural, human language) you follow the rules when you
  adhere to the grammar. A main component of a formally defined grammar is the
  set of productions --- the rules that /produce/ valid members of the language.
  In essence, applying these rules is how different strings of words (phrases,
  sentences) are generated. Deliberately, or even ignorantly violating these
  rules leads to confusion and failed communication. Because the grammatical
  rules of English are so complex, ambiguity and misunderstanding frequently
  lurk beneath the surface.

  Take what you said earlier, Ila. "I know programming languages facilitate X".
  (I\rsquo{}m abbreviating.) One sentence, or two?

  I know programming. Languages facilitate X.

  Abu said, it seems pretty clear from the context that it\rsquo{}s one sentence. Ila
  said, I agree. I don\rsquo{}t see the problem.

  It\rsquo{}s NOT a problem --- for you. Missing the context, and minus the fluidity
  and rapidity with which you uttered that ONE sentence, however, it could be
  misconstrued --- legally but erroneously parsed --- into two.

  Take this imperative statement: Love One Another. A Christian maxim, if I\rsquo{}m
  not mistaken.

  Ila and Abu glanced at each other in surprise. Til, til now, had never brought
  up religion in any of their discussions, whereas *they* had certainly broached
  the subject, on more than one occasion, when it was just the two of them,
  meeting to work on homework assignments, or just waiting for Til. Ila thought
  Abu, a Mormon, hopelessly deluded. Abu thought Ila, an evangelical Christian,
  a prospective Mormon, if she could only get past her myopia! Now to hear Til
  bring this up was a tantalizing turn, and mildly amusing to all three of them
  when Ila and Abu said in unison, John 13:34.

  Yes, well, very good, said Til, a little taken aback. I see you both know your
  Bible. If they think, thought Til, that this is leading into a religious
  discussion, they will be disappointed. Not that he was opposed to hearing
  their religious opinions, just not right now. (When, then? asked a nagging
  voice in his head. He ignored it. But God was still to be the subject of a
  sentence.)

  This is not the simplest English grammatical construction. Simpler would be
  God Loves You, which is in Subject-Verb-Object order, very common in English.
  You could make sense of each of the other five permutations, but only by
  giving a lot of context.

  Love Another One. 

  One Love Another.

  One Another Love.

  Another Love One.

  Another One Love.

  So, said Ila, are you saying word order causes communication problems?

  Hmmm, said Abu. Have you ever tried to make sense of some kinds of poetry?
  Poets are always scrambling word order to fit some rhyme or meter scheme.
  Like in a poem by Wordsworth that I just read:

#+BEGIN_QUOTE
  Bliss was it in that dawn to be alive.
  But to be young was very Heaven!
#+END_QUOTE

** TODO Add endnote with reference

  Said Til, Or from a more familiar poem: Quoth the raven instead of The raven
  said. Still perfectly correct. In fact, saying The raven quoth is *not*
  grammatical, by the very special rule attached to that very archaic word.

  So, not so much word order as word choice. Remember I started talking about
  the misuse of English to obfuscate? *Jargon* is the imp here. And we are all
  guilty of listening to that imp and using jargon to erect barriers in the
  guise of facilitators. We may justify ourselves in this indulgence because of
  the benefits --- and there *are* benefits --- but do we ever consider the
  costs?

  Jargon is meant to impede understanding by outsiders of what insiders are
  saying. Hiding the meaning of a communication from the uninitiated.

  For example, what does partial template specialization mean? And what is the
  cardinality of the power set?

  I can tell you the answer to your second question, but I have no clue about
  the first, said Ila.

  Abu said, I agree, but regarding the second question, do you think it\rsquo{}s better
  to put it more simply, like, how many subsets does a set of size *n* have?

  I do, said Til. Math is a language that desperately needs less jargon, more
  clarity. And not to keep you in suspense, well, not to keep you from the
  adventure of discovery with a little hint, partial template specialization is
  a very obscure "feature" of the C++ language.

  You have no doubt heard the stories about when a business consultant, tongue
  firmly in cheek --- or not --- randomly chooses three words from three
  different lists to create for client consideration impressive-sounding,
  content-free phrases, like

  customer value trajectory, or stratified business intelligence, or hypercubic
  mission criticality.

  Wow, did you just make those up? wondered Abu, silently. Ila said, I hear that
  kind of jargon all the time from the consultants my company hires. It\rsquo{}s all
  nonsense, if you ask me.

  But not all of it is intentional, said Til. Let me put it this way.
  Proclivities, what a nice word! Many people have proclivities, inclinations,
  predispositions to use more words, or bigger words, or "shinier" words than
  necessary to get what they want. Flattery is replete with this abuse of
  language.

  Abu rose to the challenge: Your mellifluous speech shows a penchant for
  pulchritudinous word marshalling.

  Marshmallowing, you mean, chimed in Ila. Sicky sweet, with no nutritional
  value!

  So you agree it\rsquo{}s a problem! winked Til. Both Abu and Ila nodded and chuckled.

  Well, it\rsquo{}s not one we\rsquo{}re going to solve today, he said. So let\rsquo{}s go back to
  talking about problems in mathematics. Mathematical language, unlike natural
  language, is precise and unambiguous. Equations --- tautologies --- always
  true. Never a doubt. Pure syntax without the clouding confusion of semantics.

  That\rsquo{}s the official story. Now let me qualify that a bit. One of the, if not
  *the* most brilliant mathematical logicians of all time, Kurt G\ouml{}del, once
  said, "The more I think about language, the more it amazes me that people ever
  understand each other." What amazes me about mathematicians, who are people
  too, is that they are such poor writers --- when writing mathematics, at
  least. I alluded to this a few minutes ago. Math writing is notorious for its
  lack of clarity, despite its claim of delivering unadulterated truth. (Donald
  Knuth has a great example of the contrast. Endnote, take from Celebrate
  Clarity document.)

  Think about what you know about the language of logic. It had something of a
  learning curve when you first encountered it, right? Formal logic is a
  formidable but foundational system of thought, a way to give /precision/ to
  thought and reasoning, that can nonetheless trip up the unwary. Since I
  mentioned Kurt G\ouml{}del earlier, let me give you a description of formal systems,
  or at least, the /rules/ of formal systems, from the book /Incompleteness: The
  Proof and Paradox of Kurt G\ouml{}del/, by Rebecca Goldstein:

*** TODO Put in the endnote
     on page 86.

  This passage pauses while expressing the point of view that \ldquo{}mathematics is
  merely syntactic;\rdquo

#+BEGIN_QUOTE
  its truth derives from the rules of formal systems, which are of three basic
  sorts: the rules that specify what the symbols of the system are (its
  \ldquo{}alphabet\rdquo); the rules that specify how the symbols can be put together into
  what are called well-formed formulas, standardly abbreviated \ldquo{}wff,\rdquo and
  pronounced \ldquo{}woof\rdquo; and the rules of inference that specify which wffs can be
  derived from which.
#+END_QUOTE

** TODO Have Abu and/or Ila interject some comment here.
   Relate wffs to what they have seen before. And what they will see hereafter.

  While obviously mathematical in nature, indeed, *discrete* mathematical, let\rsquo{}s
  narrow our problems focus to problems in computer science.

  Computer scientists, especially theoretical computer scientists, like to cast
  problems into the common mold of languages. They do this for technical reasons,
  more thoroughly delved into in a course on computational theory. But here is a
  simple, favorite example: Is 23 prime? This is a decision problem whose answer
  is yes, easily verified by simply trying to divide 23 by 2 and 3, and failing on
  both counts, of course. This decision could *also* be made by sequentially
  searching for and finding the string "23" in the set of strings ["2" "3" "5"
  "7" "11" "13" "17" "19 "23" ...]. 

** TODO Interject an Exercise
   Why do we not need to do trial division of 23 by 5, 7, 11, etc., to clinch
   its primeness?

#+BEGIN_SRC emacs-lisp
  (format "%S" (number-to-string 23))
#+END_SRC

#+RESULTS:
: "23"

#+BEGIN_SRC emacs-lisp :results raw
  (format "%S" (member (number-to-string 23)
                       (map 'list 'number-to-string [2 3 5 7 11 13 17 19 23])))
#+END_SRC

#+RESULTS:
("23")

  This set of strings is a language, and if you allow that the \ldquo{}...\rdquo stands for
  an infinity of bigger and bigger strings of this rather well-known kind, it is
  the language of PRIMES. It is given the name PRIMES, at any rate. So, does
  PRIMES contain the string "23232323232323232323"? is another way to ask, is
  23232323232323232323 prime? The answer is no --- it\rsquo{}s a composite number with
  seven prime factors --- including 23 --- but the computational solution to
  that set membership determination problem is significantly harder than the one
  for 23. It\rsquo{}s not done by simply searching in a static list. While many lists
  of primes exist, no one creates lists with every prime in it up to some huge
  limit. True, programs exist that can do that, using some variation of the
  classic Sieve of Eratosthenes, which goes *way* back, showing how old this
  problem is. But the point is, to solve a language membership problem you need
  computational strategies and tactics and resources. Simply put, we can /model
  computation/ most generally in terms of machinery that can input a string, and
  output a \ldquo{}yes\rdquo or a \ldquo{}no\rdquo --- \ldquo{}in the language\rdquo, or \ldquo{}not\rdquo.

  Ila said, But not every problem has a yes-or-no answer, and Abu agreed,
  offering "Like sorting, which I understand to be a typical problem for
  computers."

  Ah, my young friends, Til chuckled. It so happens you are right, but computer
  scientists are clever people, and they have figured out a way to model a very
  large number of problems *as* decision problems, or as a series of decision
  problems. Your very example of sorting, Abu, is one of the easiest.

  How so?, said Abu, exchanging a puzzled look with Ila.

  Look at a simple example. Sorting =(13 2 26)= in ascending order is a matter
  of answering three questions: is 13 less than 2 (no, so swap them), is 2 less
  than 26 (yes, so don\rsquo{}t swap them), and, is 13 less than 26 (yes, so leave them
  where they are as well). The result: =(2 13 26)=.

#+BEGIN_SRC emacs-lisp :results raw
  (let* ((unsorted '(13 2 26))
         (a (nth 0 unsorted))
         (b (nth 1 unsorted))
         (c (nth 2 unsorted)))
    (if (< a b)
        (if (< a c)
            (if (< b c)
                (list a b c)
              (list a c b))
          (list c a b))
      (if (< b c)
          (if (< a c)
              (list b a c)
            (list b c a))
        (list c b a))))
#+END_SRC

#+RESULTS:
(2 13 26)

  Ila was still puzzled. "How does that relate to a set membership decision problem?"
  Abu grinned his big, I think I know grin, and said: Let me try to answer that.
  Til said, Go ahead! as Ila gritted her teeth. She thought she knew how now too.

  In the realm of integers, I can take the /language/ ["1" "2" "3" "4" "5" "6"
  ...] and split it up into subsets like so:

  less-than-2: ["1"]

  less-than-3: ["1" "2"]

  less-than-4: ["1" "2" "3"]

  and so on, as many as I like. Then for the question, is a < b, just ask is a
  in the subset less-than-b?

  Ila frowned. But isn\rsquo{}t that a way, way inefficient way to compare two numbers?
  Til said, Yes, it is, but if we\rsquo{}re not concerned with efficiency, that
  approach certainly works.

  But consider a big advantage of treating numbers as strings of digits. As you
  know, when the numbers get big we need special procedures if we want to do
  arithmetic with them. Let\rsquo{}s lump the relational operations with the arithmetic
  ones, and ask, how would one answer a simple =a < b= question, given:

#+BEGIN_SRC emacs-lisp :results silent
  (setq a-as-string "361070123498760381765950923497698325576139879587987251757151" 
        b-as-string "36107058266725245759262937693558834387849309867353286761847615132153745")
#+END_SRC
 
#+BEGIN_SRC emacs-lisp :results raw
  (< (length a-as-string) (length b-as-string))   
#+END_SRC

#+RESULTS:
t

  That\rsquo{}s easy! b is bigger, because it has more digits, said Ila. Right, said
  Abu. At least, as long as the first dozen digits of b are not zeros! Ila
  nodded agreement. And even if the strings were the same length, a
  digit-by-digit comparison would soon reveal the answer. Abu quickly added, So,
  banning leading zeros in these strings-of-digits, /lexicographical/ ordering
  comes to mind as a convenient way to sort them, one that can answer all
  relative size questions. Am I right?

  Til nodded while Ila thought, Of course you are, smarty pants, then said, But
  why the jargony *lexicographical*? Isn\rsquo{}t there a better word than that?

  Abu said, I don\rsquo{}t remember where I saw that, and no, I don\rsquo{}t know of an
  another, easier way to say what it means. What, technically speaking, *does*
  it mean, Til?

  \ldquo{}You\rsquo{}re about to find out!\rdquo Til smiled, as he padded them some exercises.

* TODO Flesh Out
  Include examples of state diagrams as graphs, and derivations as abstract
  syntax trees. Simple models of finite-state automata. An example of a 1-bit
  computer (with two states).

  From the book /Incompleteness: The Proof and Paradox of Kurt G\ouml{}del/, by
  Rebecca Goldstein, it is on page 110, where G\ouml{}del is quoted as saying, "The
  more I think about language, the more it amazes me that people ever understand
  each other." See also page 112, at the top.
 
** ZCF 

   In normal usage, a language is something we use to communicate, in speaking
   or writing. In theoretical computer science, a language is no more and no
   less than some subset of a set of all strings over some alphabet. Formal
   definitions follow:

*** Alphabet
   
    Any non-empty, finite set (typically abbreviated \Sigma).

*** Symbols

    The members or elements of an *alphabet*.

*** String over an Alphabet

    A finite *sequence* of *symbols* from a given *alphabet*.

    Usually written side-by-side without commas. E.g., abab rather than {a, b, a, b}.
    
*** Length

    The number of *symbols* contained in a *string*.

    \vert{}w\vert denotes the length of w.
    
*** Empty String

    A *string* that has a *length* of zero. (Abbreviated \lambda or \epsilon.)
   
*** Concatenation

    The process of appending the *symbols* of one string to the end of another
    *string*, in the same order.

*** Lexicographic Ordering
    
    A method of ordering *strings* that sorts them first by *length* (with
    shorter *strings* coming first) and then by predefined order of the
    *symbols* as given in association with a particular *alphabet*.

** Grammar 

  What is a grammar?
  
** Formal Definition

   A *Phrase-Structure Grammar* (PSG) is a four-tuple:

   G = (V, T, S, P) where

   - V is a set of Variables (Nonterminals)
   - T is a set of Terminals (V \cap T = \emptyset)
   - S is the Start variable (S \in V)
   - P is a finite set of Productions (Rules), each one mapping a Variable to
     a string of Variables and Terminals.

** A Familiar Example

   Consider this PSG for a (super small) subset of the English language:

   V = [SENTENCE NOUN-PHRASE VERB-PHRASE ARTICLE ADJECTIVE NOUN VERB ADVERB]

   T = [the hungry sleepy cat dog chases runs quickly slowly]

*** Rules for the Grammar

    | P = [ |             |   |                                     |
    |       | SENTENCE    | \rightarrow | NOUN-PHRASE VERB-PHRASE NOUN-PHRASE |
    |       | SENTENCE    | \rightarrow | NOUN-PHRASE VERB-PHRASE             |
    |       | NOUN-PHRASE | \rightarrow | ARTICLE ADJECTIVE NOUN              |
    |       | NOUN-PHRASE | \rightarrow | ARTICLE NOUN                        |
    |       | VERB-PHRASE | \rightarrow | VERB-PHRASE ADVERB                  |
    |       | VERB-PHRASE | \rightarrow | VERB                                |
    |       | ARTICLE     | \rightarrow | the \vert \lambda                             |
    |       | ADJECTIVE   | \rightarrow | hungry \vert sleepy                     |
    |       | NOUN        | \rightarrow | cat \vert dog                           |
    |       | VERB        | \rightarrow | chases \vert runs                       |
    |       | ADVERB      | \rightarrow | slowly \vert quickly                    |
    | ]     |             |   |                                     |

*** Derivation

  The process of producing a sequence of terminals from the Start Variable by
  replacing variables one at a time by applying some Rule is called /Derivation/.

**** Example Derivation

   | SENTENCE | \rightarrow | NOUN-PHRASE VERB-PHRASE            |
   |          | \rightarrow | ARTICLE ADJECTIVE NOUN VERB-PHRASE |
   |          | \rightarrow | ARTICLE ADJECTIVE NOUN VERB        |
   |          | \rightarrow | the ADJECTIVE NOUN VERB            |
   |          | \rightarrow | the sleepy NOUN VERB               |
   |          | \rightarrow | the sleepy dog VERB                |
   |          | \rightarrow | the sleepy dog runs                |
 
   Using the above example as a guide, find derivations for each of the
   following sentences:

**** 1
    the hungry cat runs slowly

**** 2
    the sleepy cat chases quickly

**** 3
    the sleepy cat chases the hungry dog

*** Sample Code

#+BEGIN_SRC emacs-lisp :results silent
  (setq es ""
        productions
        '((SENTENCE NOUN-PHRASE VERB-PHRASE NOUN-PHRASE)
          (SENTENCE NOUN-PHRASE VERB-PHRASE)
          (NOUN-PHRASE ARTICLE ADJECTIVE NOUN)
          (NOUN-PHRASE ARTICLE NOUN)
          (VERB-PHRASE VERB-PHRASE ADVERB)
          (VERB-PHRASE VERB)
          (ARTICLE the es)
          (ADJECTIVE hungry sleepy)
          (NOUN cat dog)
          (VERB chases runs)
          (ADVERB slowly quickly))
        reverse-productions (reverse productions))

  (defun nonterminals-remain (derivation)
    (and (listp derivation)
         (let* ((before (mapcar 'symbol-name derivation))
                (after (mapcar 'upcase before)))
           (intersection before after :test 'string=))))

  (defun derive (LHS)
    (let* ((rules (if (zerop (random 2)) productions reverse-productions))
           (RHS (cdr (assoc LHS rules))))
      (if (null RHS)
          (list LHS)
        (if (nonterminals-remain RHS)
            RHS
          (list (nth (random (length RHS)) RHS))))))

  (defun transform-terminal (terminal)
    (or (and (boundp terminal) (symbol-value terminal))
        (symbol-name terminal)))

  (defun find-derivation (start-symbol)
    (let ((derivation (list start-symbol)))
      (while (nonterminals-remain derivation)
        (setq derivation (apply 'append (mapcar 'derive derivation))))
      (mapconcat 'transform-terminal derivation " ")))
#+END_SRC 

#+BEGIN_SRC emacs-lisp
  (find-derivation 'SENTENCE)
#+END_SRC

#+RESULTS:
: the cat chases  hungry dog

#+RESULTSg:
: the dog chases quickly quickly quickly

#+RESULTSg:
: the sleepy dog chases the sleepy cat

** Question

   How many different sentences can repeated evaluations of =(find-derivation
   'SENTENCE)= find?

** Question

   With these rules is there a derivation for this?

   =the hungry sleepy dog runs=

*** Answer

    No.

**** So how would you fix that?

     Add a *loopy* rule!
 
     ADJECTIVE \rightarrow ADJECTIVE ADJECTIVE \vert \lambda

* A Challenge

  What rules would you need to change or add to generate this sentence?

  =the quick brown fox jumps over the lazy dog=

** TODO Answer

   ADJECTIVE \rightarrow hungry \vert sleepy \vert quick \vert brown \vert lazy

   PREPOSITION \rightarrow of \vert from \vert by \vert on \vert in \vert over \vert \dots

   PREPOSITIONAL-PHRASE \rightarrow PREPOSITION NOUN-PHRASE

   VERB-PHRASE \rightarrow VERB PREPOSITIONAL-PHRASE

#+BEGIN_SRC emacs-lisp
  (setq parsed [S [NP [ART the] [ADJ [ADJ quick] [ADJ brown]] [N
        fox]] [VP [V jumps] [PP [P over] [NP [ART the] [ADJ lazy]
        [N dog]]]]])
  (kill-new (format "%s" parsed))
#+END_SRC

*** Visualize Derivation

    The derivation of a valid syntactic "sentence" can be visualized as the
    process of building a *syntax tree* (AKA a *parse tree*).

    See http://www.ironcreek.net/phpsyntaxtree/.

* A Harder Challenge

  Go back to the original Grammar.

  Replace these three rules:

  ADJECTIVE \rightarrow Buffalo

  NOUN \rightarrow buffalo

  VERB \rightarrow buffalo

  With these new rules, is there a derivation for this "sentence"?!

** This is a sentence?!
   Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo

*** Meaning Explained
  (The) Buffalo buffalo (that) Buffalo buffalo (often) buffalo (in turn) buffalo
  (other) Buffalo buffalo.

* Fancy Nouns

  Fancy nouns are *nested* nouns, for example "the fresh brownies that the
  little rascals without permission devoured" --- which could be rephrased as
  "the little rascals without permission devoured the fresh brownies, and it\rsquo{}s
  these brownies I want to focus on."

  So, a nested noun is a nested noun followed by a relative pronoun (e.g.,
  /that/) followed by a verb followed by a nested noun,

  OR,

  it\rsquo{}s a nested noun followed by a relative pronoun followed by a nested noun
  followed by a verb,

  OR,

  it\rsquo{}s a nested noun followed by a preposition followed by a nested noun,

  OR,

  it\rsquo{}s just an article followed by any number of adjectives followed by a plain
  old (non-nested) noun!

* Nested Nouns
 
  NESTED-NOUN \rightarrow NESTED-NOUN RELATIVE-PRONOUN VERB NESTED-NOUN

  NESTED-NOUN \rightarrow NESTED-NOUN RELATIVE-PRONOUN NESTED-NOUN VERB

  NESTED-NOUN \rightarrow PREPOSITION NESTED-NOUN

  NESTED-NOUN \rightarrow ARTICLE NOUN-PHRASE

  NOUN-PHRASE \rightarrow ADJECTIVE NOUN-PHRASE

  NOUN-PHRASE \rightarrow NESTED-NOUN

  NOUN-PHRASE \rightarrow NOUN
 
  ARTICLE \rightarrow a \vert an \vert the \vert \lambda

  RELATIVE-PRONOUN \rightarrow that \vert \lambda

  PREPOSITION \rightarrow of \vert from \vert by \vert \dots

** Now It\rsquo{}s Possible

   Let NN = NESTED-NOUN, RP = RELATIVE-PRONOUN, es = \lambda (the empty string).

#+BEGIN_SRC emacs-lisp
  (setq parsed [S [NP [NN [NN [ART es] [NP [ADJ Buffalo] [NP [N
        buffalo]]]] [RP es] [NN [NP [ADJ Buffalo] [NP [N buffalo]]]][V
        buffalo]]] [VP [V buffalo]] [NP [ADJ Buffalo] [NP [N buffalo]]]])

  (kill-new (format "%s" parsed))
#+END_SRC

* What is the Context?

  The grammar for English is "Context Free". By way of contrast, here\rsquo{}s an
  example of two productions in a NON-Context-Free grammar:

  aAc \rightarrow aabc

  aAd \rightarrow abad

  Note that A\rsquo{}s expansion is different when it\rsquo{}s surrounded by a and c than when
  it\rsquo{}s surrounded by a and d. We say A\rsquo{}s interpretation has context
  "sensitivity". A Grammar/Language with this feature is called
  Context-Sensitive.

* Regular Languages

  Moving down to the simplest type, a language is /regular/ *iff* some /regular
  expression/ describes it.

  Regular expressions use the so-called regular operations (\cup, \circ, and \star) ---
  (union, concatenation, and star) --- to build regular languages. Here is a
  recursive definition:

  R is a *regular expression* (an *re* for short) if R is any of

  - \emptyset
  - {\lambda}
  - {a} for some a \in \Sigma
  - R_1 \cup R_2, where R_1 and R_2 are *re*\rsquo{}s
  - R_1 \circ R_2, where R_1 and R_2 are *re*\rsquo{}s
  - R^{\star}, where R is an *re*
 
  Some shorthand:

  - a \equiv \{a\}
  - \lambda \equiv \{\lambda\}
  - R^{\plus} \equiv R \circ R^{\star}
  - R^{\star} \equiv R^{\plus} \cup \lambda
  - R^k \equiv R \circ R \circ R \circ \dots \circ R (k times)

  Note: R \circ R is usually written without the \circ, i.e., RR. In this way \circ is
  analogous to the multiplication operator.

* Forward Exercises

  What language is generated by a given grammar?

   Let V = {S, A, B} and T = {a, b}. Find the language generated by the
   grammar (V, T, S, P) when the set P of productions consists of each of the
   following:

*** 1

    S \rightarrow AB

    A \rightarrow ab

    B \rightarrow bb

*** 2

    S \rightarrow AB

    S \rightarrow aA

    A \rightarrow a

    B \rightarrow ba

*** 3

    S \rightarrow AB

    S \rightarrow AA

    A \rightarrow aB

    A \rightarrow ab

    B \rightarrow b

*** 4

    S \rightarrow AA

    S \rightarrow B

    A \rightarrow aaA

    A \rightarrow aa

    B \rightarrow bB

    B \rightarrow b

*** 5

    S \rightarrow AB

    A \rightarrow aAb

    B \rightarrow bBa

    A \rightarrow \lambda

    B \rightarrow \lambda

#+BEGIN_SRC emacs-lisp
  (setq es ""
        productions
        '((S A B)
          (A a A b)
          (B b B a)
          (A es)
          (B es))
        reverse-productions (reverse productions))
#+END_SRC

#+BEGIN_SRC emacs-lisp
  (find-derivation 'S)
#+END_SRC

** Reverse Exercises

  What grammar generates a given language?

*** 1

    Construct a PSG to generate {0^{2n}1 \vert n \ge 0}.

*** 2

    Construct a PSG to generate {0^{n}1^{2n} \vert n \ge 0}.

*** 3

    Construct a PSG to generate {0^n 1^m 0^n \vert m \ge 0 and n \ge 0}.

** ILO  

   Noam Chomsky is a linguist who first proposed the hierarchical language
   classification scheme that now bears his name.

*** The Chomsky Hierarchy

: Universal Set of All Languages (the superset of Types 0-3)
:   +------------------------------------------------------+
:   |   Type 0 Recursively Enumerable Languages            |
:   |   +----------------------------------------------+   |
:   |   |    Type 1 Context Sensitive Languages        |   |
:   |   |    +-------------------------------------+   |   |
:   |   |    |   Type 2 Context Free Languages     |   |   |
:   |   |    |   +-----------------------------+   |   |   |
:   |   |    |   |  Type 3 Regular Languages   |   |   |   |
:   |   |    |   |                             |   |   |   |
:   |   |    |   |                             |   |   |   |
:   |   |    |   +-----------------------------+   |   |   |
:   |   |    |                                     |   |   |
:   |   |    +-------------------------------------+   |   |
:   |   |                                              |   |
:   |   +----------------------------------------------+   |
:   |                                                      |
:   +------------------------------------------------------+

*** A Tabular Taxonomy

   The following table maps the notions of language classes with the types of
   grammars that can generate those languages. The restrictions on productions
   distinguish what\rsquo{}s what (where N = Nonterminal, tl = terminal, LHS =
   Left-Hand Side, RHS = Right-Hand Side).

   | Language Class         | Type | Restrictions on Grammar Productions       |
   |------------------------+------+-------------------------------------------|
   | Regular                |    3 | Left-linear or Right-linear               |
   |                        |      | (each RHS must be either a tl or \lambda,       |
   |                        |      | or have a single Nonterminal and be       |
   |                        |      | all like Ntl, or all like tlN).           |
   |                        |      |                                           |
   | Context Free           |    2 | Each LHS must have only one Nonterminal.  |
   |                        |      |                                           |
   | Context Sensitive      |    1 | LHS may have more than one Nonterminal,   |
   |                        |      | but the length of the LHS must be         |
   |                        |      | at most the length of the RHS             |
   |                        |      | (except for S \rarr \lambda productions).           |
   |                        |      |                                           |
   | Recursively Enumerable |    0 | No restrictions                           |
   |                        |      | (length of LHS may exceed length of RHS). |

*** TODO Redo These Classification Exercises

   Can you distinguish grammar types?

   Let V = {S, A, B}, T = {a, b}, and G = (V, T, S, P) (P to be given later).

   Determine whether G

   - is a type 0 grammar but not a type 1 grammar, or
   - is a type 1 grammar but not a type 2 grammar, or
   - is a type 2 grammar but not a type 3 grammar, or 
   - is a type 3 grammar,

   when P, the set of productions, is one of the following:

**** 1

    S \rightarrow aAB 

    A \rightarrow Bb

    B \rightarrow \lambda

**** 2

    S \rightarrow aA

    A \rightarrow a

    A \rightarrow b

**** 3

    S \rightarrow ABa

    AB \rightarrow a

**** 4

    S \rightarrow ABA

    A \rightarrow aB

    B \rightarrow ab

**** 5

    S \rightarrow aA

    aA \rightarrow B

    B \rightarrow aA

    A \rightarrow b

**** 6

    S \rightarrow bA

    A \rightarrow b

    S \rightarrow \lambda

**** 7

    S \rightarrow AB

    B \rightarrow aAb

    aAb \rightarrow b

* TODO For an Epilogue

  Solving a math problem to get clues to Til\rsquo{}s unknown whereabouts is a fitting
  conclusion, while also foreshadowing further adventures/installments of TIA
  interaction.

  Til has gone missing for two weeks. He knows where he is, but has no way to
  communicate his location in the desert where he went to seek solitude.
  Something he feels compelled to do from time to time, much to his wife\rsquo{}s
  chagrin. This time he is in some kind of trouble, trapped without means to get
  out on his own.

  The problem is, his tracer (read GPS) signal is encrypted, in a very eccentric
  way. This way may have something to do with the final exercise/problem/puzzle
  he gave Abu and Ila, namely to find the connection between Edgar Allan Poe and
  the phrase "Notice cousin Felipe".
